{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-19T02:23:51.900511Z","iopub.execute_input":"2024-09-19T02:23:51.900825Z","iopub.status.idle":"2024-09-19T02:23:52.256311Z","shell.execute_reply.started":"2024-09-19T02:23:51.900792Z","shell.execute_reply":"2024-09-19T02:23:52.255549Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# !sudo apt-get update  \n# !sudo apt-get install -y build-essential libopenmpi-dev\n# !pip install torch\n!pip install datasets\n!pip install transformers\n!pip install peft\n!pip install trl\n!pip install accelerate\n!pip install bitsandbytes\n!pip install sentencepiece\n# !pip install pyarrow==16.0.0 # 需重启会话\n\n# !pip install setfit==1.0.3\n# !pip install lingua-language-detector==2.0.2\n!pip install polars==0.20.31\n\n# !pip install google-cloud-bigquery==3.24.0\n# !pip install shapely==2.0.4\n# !pip install deepspeed\n# !pip install mpi4py\n!pip install evaluate\n!pip install rouge_score","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-19T02:23:52.258214Z","iopub.execute_input":"2024-09-19T02:23:52.258841Z","iopub.status.idle":"2024-09-19T02:26:02.300681Z","shell.execute_reply.started":"2024-09-19T02:23:52.258792Z","shell.execute_reply":"2024-09-19T02:26:02.299565Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nCollecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.12.0\nCollecting trl\n  Downloading trl-0.10.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.4.0)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.44.0)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl) (1.26.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl) (0.33.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (2.21.0)\nCollecting tyro>=0.5.11 (from trl)\n  Downloading tyro-0.8.11-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.24.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (4.66.4)\nRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.1)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl) (5.9.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (2024.7.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\nDownloading trl-0.10.1-py3-none-any.whl (280 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.1/280.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.8.11-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nInstalling collected packages: shtab, tyro, trl\nSuccessfully installed shtab-1.7.1 trl-0.10.1 tyro-0.8.11\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.33.0)\nRequirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.24.6)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.3\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nCollecting polars==0.20.31\n  Downloading polars-0.20.31-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\nDownloading polars-0.20.31-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: polars\n  Attempting uninstall: polars\n    Found existing installation: polars 1.5.0\n    Uninstalling polars-1.5.0:\n      Successfully uninstalled polars-1.5.0\nSuccessfully installed polars-0.20.31\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.21.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m46.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/huggingface/transformers\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T02:26:02.301967Z","iopub.execute_input":"2024-09-19T02:26:02.302260Z","iopub.status.idle":"2024-09-19T02:26:23.027653Z","shell.execute_reply.started":"2024-09-19T02:26:02.302228Z","shell.execute_reply":"2024-09-19T02:26:23.026650Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'transformers'...\nremote: Enumerating objects: 230049, done.\u001b[K\nremote: Counting objects: 100% (38044/38044), done.\u001b[K\nremote: Compressing objects: 100% (2532/2532), done.\u001b[K\nremote: Total 230049 (delta 37111), reused 35643 (delta 35466), pack-reused 192005 (from 1)\u001b[K\nReceiving objects: 100% (230049/230049), 235.44 MiB | 30.43 MiB/s, done.\nResolving deltas: 100% (168540/168540), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/transformers\n# 这意味着该目录下有一个setup.py文件，其中包含了项目的安装配置\n%pip install .","metadata":{"execution":{"iopub.status.busy":"2024-09-19T02:26:23.030821Z","iopub.execute_input":"2024-09-19T02:26:23.031236Z","iopub.status.idle":"2024-09-19T02:27:07.524509Z","shell.execute_reply.started":"2024-09-19T02:26:23.031191Z","shell.execute_reply":"2024-09-19T02:27:07.523398Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/working/transformers\nProcessing /kaggle/working/transformers\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.4.4)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.45.0.dev0) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (2024.7.4)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9761399 sha256=d0eb8bd8f3244f29005d082066282e9705de91da1184eaa6d7c5fdf69b0e30f8\n  Stored in directory: /tmp/pip-ephem-wheel-cache-fin9fsvt/wheels/7e/b2/24/0b3be37b3b423a6f2fd25fd6368a1f4b0888942789c7e68bc6\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.0\n    Uninstalling transformers-4.44.0:\n      Successfully uninstalled transformers-4.44.0\nSuccessfully installed transformers-4.45.0.dev0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/transformers/examples\n%pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-09-19T02:27:07.525895Z","iopub.execute_input":"2024-09-19T02:27:07.526222Z","iopub.status.idle":"2024-09-19T02:27:09.354646Z","shell.execute_reply.started":"2024-09-19T02:27:07.526185Z","shell.execute_reply":"2024-09-19T02:27:09.353559Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/working/transformers/examples\n\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/transformers","metadata":{"execution":{"iopub.status.busy":"2024-09-19T02:27:09.355972Z","iopub.execute_input":"2024-09-19T02:27:09.356299Z","iopub.status.idle":"2024-09-19T02:27:09.362944Z","shell.execute_reply.started":"2024-09-19T02:27:09.356263Z","shell.execute_reply":"2024-09-19T02:27:09.362060Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/working/transformers\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 使用T5模型运行文本摘要任务\n\n- The example script downloads and preprocesses a dataset from the 🤗 Datasets library. \n- Then the script fine-tunes a dataset with the Trainer on an architecture that supports summarization. ","metadata":{}},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"   # 不使用wandb可视化第三方服务","metadata":{"execution":{"iopub.status.busy":"2024-09-19T03:12:18.688699Z","iopub.execute_input":"2024-09-19T03:12:18.689402Z","iopub.status.idle":"2024-09-19T03:12:18.694157Z","shell.execute_reply.started":"2024-09-19T03:12:18.689361Z","shell.execute_reply":"2024-09-19T03:12:18.693263Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"!python examples/pytorch/summarization/run_summarization.py \\\n    --model_name_or_path google-t5/t5-small \\\n    --do_train \\\n    --do_eval \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --source_prefix \"summarize: \" \\\n    --output_dir /kaggle/workspace/output/tst-summarization \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --overwrite_output_dir \\\n    --predict_with_generate","metadata":{"execution":{"iopub.status.busy":"2024-09-19T03:12:25.159657Z","iopub.execute_input":"2024-09-19T03:12:25.160464Z","iopub.status.idle":"2024-09-19T05:18:41.055894Z","shell.execute_reply.started":"2024-09-19T03:12:25.160426Z","shell.execute_reply":"2024-09-19T05:18:41.054742Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nOverwrite dataset info from restored data version if exists.\nLoading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\nFound cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d)\nLoading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n[INFO|configuration_utils.py:672] 2024-09-19 03:12:39,000 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n[INFO|configuration_utils.py:739] 2024-09-19 03:12:39,004 >> Model config T5Config {\n  \"_name_or_path\": \"google-t5/t5-small\",\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 512,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 6,\n  \"num_heads\": 8,\n  \"num_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"transformers_version\": \"4.45.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 03:12:39,091 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 03:12:39,091 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 03:12:39,091 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 03:12:39,091 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 03:12:39,091 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n[INFO|modeling_utils.py:3702] 2024-09-19 03:12:39,202 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\n[INFO|configuration_utils.py:1097] 2024-09-19 03:12:39,209 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\n[INFO|modeling_utils.py:4544] 2024-09-19 03:12:39,431 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\n[INFO|modeling_utils.py:4552] 2024-09-19 03:12:39,432 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google-t5/t5-small.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n[INFO|configuration_utils.py:1052] 2024-09-19 03:12:39,524 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\n[INFO|configuration_utils.py:1097] 2024-09-19 03:12:39,524 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nLoading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-0c98e5f34d664d79.arrow\nLoading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-b70318085e7e41c4.arrow\n[INFO|trainer.py:2212] 2024-09-19 03:12:41,268 >> ***** Running training *****\n[INFO|trainer.py:2213] 2024-09-19 03:12:41,268 >>   Num examples = 287,113\n[INFO|trainer.py:2214] 2024-09-19 03:12:41,268 >>   Num Epochs = 3\n[INFO|trainer.py:2215] 2024-09-19 03:12:41,268 >>   Instantaneous batch size per device = 4\n[INFO|trainer.py:2217] 2024-09-19 03:12:41,269 >>   Training with DataParallel so batch size has been adjusted to: 8\n[INFO|trainer.py:2218] 2024-09-19 03:12:41,269 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n[INFO|trainer.py:2219] 2024-09-19 03:12:41,269 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:2220] 2024-09-19 03:12:41,269 >>   Total optimization steps = 107,670\n[INFO|trainer.py:2221] 2024-09-19 03:12:41,270 >>   Number of trainable parameters = 60,506,624\n  0%|                                                | 0/107670 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.9694, 'grad_norm': 1.928574562072754, 'learning_rate': 4.976780904615956e-05, 'epoch': 0.01}\n  0%|▏                                  | 500/107670 [03:44<13:27:08,  2.21it/s][INFO|trainer.py:3674] 2024-09-19 03:16:26,012 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-500\n[INFO|configuration_utils.py:407] 2024-09-19 03:16:26,015 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 03:16:26,015 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 03:16:26,515 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 03:16:26,518 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 03:16:26,518 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 03:16:26,520 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-500/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8983, 'grad_norm': 1.6127852201461792, 'learning_rate': 4.953561809231913e-05, 'epoch': 0.03}\n  1%|▎                                 | 1000/107670 [07:30<13:23:53,  2.21it/s][INFO|trainer.py:3674] 2024-09-19 03:20:11,550 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-1000\n[INFO|configuration_utils.py:407] 2024-09-19 03:20:11,553 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-1000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 03:20:11,553 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-1000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 03:20:12,057 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-1000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 03:20:12,060 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-1000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 03:20:12,061 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-1000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 03:20:12,062 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-1000/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8973, 'grad_norm': 1.7233318090438843, 'learning_rate': 4.9303427138478686e-05, 'epoch': 0.04}\n  1%|▍                                 | 1500/107670 [11:15<13:08:20,  2.24it/s][INFO|trainer.py:3674] 2024-09-19 03:23:57,057 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-1500\n[INFO|configuration_utils.py:407] 2024-09-19 03:23:57,060 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-1500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 03:23:57,060 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-1500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 03:23:57,561 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-1500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 03:23:57,564 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-1500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 03:23:57,564 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-1500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 03:23:57,565 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-1500/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.9093, 'grad_norm': 2.0341711044311523, 'learning_rate': 4.9071236184638245e-05, 'epoch': 0.06}\n  2%|▋                                 | 2000/107670 [15:01<13:02:14,  2.25it/s][INFO|trainer.py:3674] 2024-09-19 03:27:42,576 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-2000\n[INFO|configuration_utils.py:407] 2024-09-19 03:27:42,578 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-2000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 03:27:42,579 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-2000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 03:27:43,074 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-2000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 03:27:43,077 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-2000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 03:27:43,077 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-2000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 03:27:43,078 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-2000/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8974, 'grad_norm': 1.913994312286377, 'learning_rate': 4.883904523079781e-05, 'epoch': 0.07}\n  2%|▊                                 | 2500/107670 [18:47<13:10:42,  2.22it/s][INFO|trainer.py:3674] 2024-09-19 03:31:28,379 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-2500\n[INFO|configuration_utils.py:407] 2024-09-19 03:31:28,382 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-2500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 03:31:28,382 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-2500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 03:31:28,880 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-2500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 03:31:28,883 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-2500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 03:31:28,884 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-2500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 03:31:28,885 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-2500/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8627, 'grad_norm': 1.9351284503936768, 'learning_rate': 4.860685427695737e-05, 'epoch': 0.08}\n  3%|▉                                 | 3000/107670 [22:32<12:57:53,  2.24it/s][INFO|trainer.py:3674] 2024-09-19 03:35:13,716 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-3000\n[INFO|configuration_utils.py:407] 2024-09-19 03:35:13,719 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-3000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 03:35:13,719 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-3000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 03:35:14,217 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-3000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 03:35:14,220 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-3000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 03:35:14,221 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-3000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 03:35:14,222 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-3000/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8737, 'grad_norm': 1.9051710367202759, 'learning_rate': 4.837466332311693e-05, 'epoch': 0.1}\n  3%|█                                 | 3500/107670 [26:17<12:58:59,  2.23it/s][INFO|trainer.py:3674] 2024-09-19 03:38:58,859 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-3500\n[INFO|configuration_utils.py:407] 2024-09-19 03:38:58,861 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-3500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 03:38:58,862 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-3500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 03:38:59,368 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-3500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 03:38:59,371 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-3500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 03:38:59,372 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-3500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 03:38:59,373 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-3500/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8827, 'grad_norm': 1.643359899520874, 'learning_rate': 4.81424723692765e-05, 'epoch': 0.11}\n  4%|█▎                                | 4000/107670 [30:03<12:56:06,  2.23it/s][INFO|trainer.py:3674] 2024-09-19 03:42:44,323 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-4000\n[INFO|configuration_utils.py:407] 2024-09-19 03:42:44,326 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-4000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 03:42:44,326 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-4000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 03:42:44,830 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-4000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 03:42:44,833 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-4000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 03:42:44,834 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-4000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 03:42:44,835 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-4000/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8883, 'grad_norm': 1.7819279432296753, 'learning_rate': 4.791028141543606e-05, 'epoch': 0.13}\n  4%|█▍                                | 4500/107670 [33:48<12:51:40,  2.23it/s][INFO|trainer.py:3674] 2024-09-19 03:46:29,677 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-4500\n[INFO|configuration_utils.py:407] 2024-09-19 03:46:29,681 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-4500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 03:46:29,681 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-4500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 03:46:30,226 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-4500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 03:46:30,229 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-4500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 03:46:30,229 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-4500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 03:46:30,231 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-4500/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8768, 'grad_norm': 1.485243558883667, 'learning_rate': 4.767809046159562e-05, 'epoch': 0.14}\n  5%|█▌                                | 5000/107670 [37:34<12:51:29,  2.22it/s][INFO|trainer.py:3674] 2024-09-19 03:50:15,302 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-5000\n[INFO|configuration_utils.py:407] 2024-09-19 03:50:15,304 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-5000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 03:50:15,305 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-5000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 03:50:15,827 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-5000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 03:50:15,830 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-5000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 03:50:15,831 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-5000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 03:50:15,832 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-5000/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8729, 'grad_norm': 1.9264132976531982, 'learning_rate': 4.7445899507755184e-05, 'epoch': 0.15}\n  5%|█▋                                | 5500/107670 [41:19<12:45:53,  2.22it/s][INFO|trainer.py:3674] 2024-09-19 03:54:00,499 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-5500\n[INFO|configuration_utils.py:407] 2024-09-19 03:54:00,502 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-5500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 03:54:00,502 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-5500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 03:54:01,013 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-5500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 03:54:01,016 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-5500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 03:54:01,016 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-5500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 03:54:01,017 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-5500/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8741, 'grad_norm': 2.0560693740844727, 'learning_rate': 4.721370855391474e-05, 'epoch': 0.17}\n  6%|█▉                                | 6000/107670 [45:04<12:41:25,  2.23it/s][INFO|trainer.py:3674] 2024-09-19 03:57:46,148 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-6000\n[INFO|configuration_utils.py:407] 2024-09-19 03:57:46,150 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-6000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 03:57:46,151 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-6000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 03:57:46,664 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-6000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 03:57:46,667 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-6000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 03:57:46,667 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-6000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 03:57:46,668 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-6000/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8569, 'grad_norm': 1.7159429788589478, 'learning_rate': 4.69815176000743e-05, 'epoch': 0.18}\n  6%|██                                | 6500/107670 [48:50<12:44:02,  2.21it/s][INFO|trainer.py:3674] 2024-09-19 04:01:31,932 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-6500\n[INFO|configuration_utils.py:407] 2024-09-19 04:01:31,935 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-6500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 04:01:31,936 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-6500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 04:01:32,446 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-6500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 04:01:32,449 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-6500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 04:01:32,450 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-6500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 04:01:32,451 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-6500/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8648, 'grad_norm': 1.6900861263275146, 'learning_rate': 4.674932664623387e-05, 'epoch': 0.2}\n  7%|██▏                               | 7000/107670 [52:36<12:30:43,  2.23it/s][INFO|trainer.py:3674] 2024-09-19 04:05:18,048 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-7000\n[INFO|configuration_utils.py:407] 2024-09-19 04:05:18,051 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-7000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 04:05:18,051 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-7000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 04:05:18,558 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-7000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 04:05:18,561 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-7000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 04:05:18,561 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-7000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 04:05:18,563 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-7000/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8656, 'grad_norm': 1.712042212486267, 'learning_rate': 4.6517135692393426e-05, 'epoch': 0.21}\n  7%|██▎                               | 7500/107670 [56:22<12:32:35,  2.22it/s][INFO|trainer.py:3674] 2024-09-19 04:09:03,765 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-7500\n[INFO|configuration_utils.py:407] 2024-09-19 04:09:03,767 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-7500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 04:09:03,768 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-7500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 04:09:04,282 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-7500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 04:09:04,285 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-7500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 04:09:04,285 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-7500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 04:09:04,287 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-7500/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8674, 'grad_norm': 1.4795019626617432, 'learning_rate': 4.6284944738552985e-05, 'epoch': 0.22}\n  7%|██▍                             | 8000/107670 [1:00:07<12:22:12,  2.24it/s][INFO|trainer.py:3674] 2024-09-19 04:12:49,152 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-8000\n[INFO|configuration_utils.py:407] 2024-09-19 04:12:49,154 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-8000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 04:12:49,155 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-8000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 04:12:49,652 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-8000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 04:12:49,655 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-8000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 04:12:49,655 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-8000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 04:12:49,656 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-8000/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8629, 'grad_norm': 2.0872631072998047, 'learning_rate': 4.605275378471255e-05, 'epoch': 0.24}\n  8%|██▌                             | 8500/107670 [1:03:52<12:18:17,  2.24it/s][INFO|trainer.py:3674] 2024-09-19 04:16:34,239 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-8500\n[INFO|configuration_utils.py:407] 2024-09-19 04:16:34,242 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-8500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 04:16:34,243 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-8500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 04:16:34,746 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-8500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 04:16:34,749 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-8500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 04:16:34,749 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-8500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 04:16:34,751 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-8500/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8501, 'grad_norm': 1.6643394231796265, 'learning_rate': 4.582056283087211e-05, 'epoch': 0.25}\n  8%|██▋                             | 9000/107670 [1:07:38<12:10:40,  2.25it/s][INFO|trainer.py:3674] 2024-09-19 04:20:19,436 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-9000\n[INFO|configuration_utils.py:407] 2024-09-19 04:20:19,439 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-9000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 04:20:19,439 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-9000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 04:20:19,965 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-9000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 04:20:19,968 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-9000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 04:20:19,968 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-9000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 04:20:19,970 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-9000/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8523, 'grad_norm': 1.6221150159835815, 'learning_rate': 4.558837187703167e-05, 'epoch': 0.26}\n  9%|██▊                             | 9500/107670 [1:11:23<12:12:27,  2.23it/s][INFO|trainer.py:3674] 2024-09-19 04:24:04,831 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-9500\n[INFO|configuration_utils.py:407] 2024-09-19 04:24:04,833 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-9500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 04:24:04,834 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-9500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 04:24:05,339 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-9500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 04:24:05,342 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-9500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 04:24:05,343 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-9500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 04:24:05,344 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-9500/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8699, 'grad_norm': 1.5895966291427612, 'learning_rate': 4.5356180923191235e-05, 'epoch': 0.28}\n  9%|██▉                            | 10000/107670 [1:15:08<12:12:35,  2.22it/s][INFO|trainer.py:3674] 2024-09-19 04:27:50,218 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-10000\n[INFO|configuration_utils.py:407] 2024-09-19 04:27:50,221 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-10000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 04:27:50,221 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-10000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 04:27:50,725 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-10000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 04:27:50,728 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-10000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 04:27:50,729 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-10000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 04:27:50,730 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-10000/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8626, 'grad_norm': 2.053483724594116, 'learning_rate': 4.5123989969350793e-05, 'epoch': 0.29}\n 10%|███                            | 10500/107670 [1:18:54<12:13:43,  2.21it/s][INFO|trainer.py:3674] 2024-09-19 04:31:35,399 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-10500\n[INFO|configuration_utils.py:407] 2024-09-19 04:31:35,401 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-10500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 04:31:35,402 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-10500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 04:31:35,902 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-10500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 04:31:35,905 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-10500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 04:31:35,906 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-10500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 04:31:35,907 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-10500/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8513, 'grad_norm': 1.8116129636764526, 'learning_rate': 4.489179901551035e-05, 'epoch': 0.31}\n 10%|███▏                           | 11000/107670 [1:22:39<11:49:37,  2.27it/s][INFO|trainer.py:3674] 2024-09-19 04:35:20,444 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-11000\n[INFO|configuration_utils.py:407] 2024-09-19 04:35:20,447 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-11000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 04:35:20,448 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-11000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 04:35:20,953 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-11000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 04:35:20,956 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-11000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 04:35:20,956 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-11000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 04:35:20,958 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-11000/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.856, 'grad_norm': 2.2546768188476562, 'learning_rate': 4.4659608061669925e-05, 'epoch': 0.32}\n 11%|███▎                           | 11500/107670 [1:26:24<11:58:39,  2.23it/s][INFO|trainer.py:3674] 2024-09-19 04:39:05,792 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-11500\n[INFO|configuration_utils.py:407] 2024-09-19 04:39:05,794 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-11500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 04:39:05,795 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-11500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 04:39:06,309 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-11500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 04:39:06,312 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-11500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 04:39:06,312 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-11500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 04:39:06,314 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-11500/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8529, 'grad_norm': 1.8625266551971436, 'learning_rate': 4.4427417107829484e-05, 'epoch': 0.33}\n 11%|███▍                           | 12000/107670 [1:30:09<11:49:06,  2.25it/s][INFO|trainer.py:3674] 2024-09-19 04:42:50,596 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-12000\n[INFO|configuration_utils.py:407] 2024-09-19 04:42:50,599 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-12000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 04:42:50,600 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-12000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 04:42:51,106 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-12000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 04:42:51,108 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-12000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 04:42:51,109 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-12000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 04:42:51,110 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-12000/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8632, 'grad_norm': 2.362730026245117, 'learning_rate': 4.419522615398904e-05, 'epoch': 0.35}\n 12%|███▌                           | 12500/107670 [1:33:54<11:49:21,  2.24it/s][INFO|trainer.py:3674] 2024-09-19 04:46:35,696 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-12500\n[INFO|configuration_utils.py:407] 2024-09-19 04:46:35,700 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-12500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 04:46:35,701 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-12500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 04:46:36,202 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-12500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 04:46:36,205 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-12500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 04:46:36,205 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-12500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 04:46:36,206 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-12500/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8505, 'grad_norm': 1.714975118637085, 'learning_rate': 4.373084424630817e-05, 'epoch': 0.38}\n 13%|███▉                           | 13500/107670 [1:41:24<11:45:09,  2.23it/s][INFO|trainer.py:3674] 2024-09-19 04:54:06,137 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-13500\n[INFO|configuration_utils.py:407] 2024-09-19 04:54:06,139 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-13500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 04:54:06,140 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-13500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 04:54:06,630 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-13500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 04:54:06,633 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-13500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 04:54:06,634 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-13500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 04:54:06,635 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-13500/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8682, 'grad_norm': 1.9657716751098633, 'learning_rate': 4.3498653292467726e-05, 'epoch': 0.39}\n 13%|████                           | 14000/107670 [1:45:09<11:39:28,  2.23it/s][INFO|trainer.py:3674] 2024-09-19 04:57:51,228 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-14000\n[INFO|configuration_utils.py:407] 2024-09-19 04:57:51,231 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-14000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 04:57:51,232 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-14000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 04:57:51,732 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-14000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 04:57:51,735 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-14000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 04:57:51,736 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-14000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 04:57:51,737 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-14000/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8476, 'grad_norm': 1.5976101160049438, 'learning_rate': 4.326646233862729e-05, 'epoch': 0.4}\n 13%|████▏                          | 14500/107670 [1:48:54<11:35:48,  2.23it/s][INFO|trainer.py:3674] 2024-09-19 05:01:35,638 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-14500\n[INFO|configuration_utils.py:407] 2024-09-19 05:01:35,641 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-14500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 05:01:35,641 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-14500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 05:01:36,151 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-14500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 05:01:36,154 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-14500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 05:01:36,155 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-14500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 05:01:36,156 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-14500/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8501, 'grad_norm': 1.6003191471099854, 'learning_rate': 4.303427138478685e-05, 'epoch': 0.42}\n 14%|████▎                          | 15000/107670 [1:52:39<11:30:47,  2.24it/s][INFO|trainer.py:3674] 2024-09-19 05:05:21,129 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-15000\n[INFO|configuration_utils.py:407] 2024-09-19 05:05:21,133 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-15000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 05:05:21,133 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-15000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 05:05:21,648 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-15000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 05:05:21,651 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-15000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 05:05:21,651 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-15000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 05:05:21,652 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-15000/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 1.8557, 'grad_norm': 1.6565194129943848, 'learning_rate': 4.280208043094641e-05, 'epoch': 0.43}\n 14%|████▍                          | 15500/107670 [1:56:27<11:36:30,  2.21it/s][INFO|trainer.py:3674] 2024-09-19 05:09:08,470 >> Saving model checkpoint to /kaggle/workspace/output/tst-summarization/checkpoint-15500\n[INFO|configuration_utils.py:407] 2024-09-19 05:09:08,473 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-15500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 05:09:08,473 >> Configuration saved in /kaggle/workspace/output/tst-summarization/checkpoint-15500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 05:09:08,995 >> Model weights saved in /kaggle/workspace/output/tst-summarization/checkpoint-15500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 05:09:08,998 >> tokenizer config file saved in /kaggle/workspace/output/tst-summarization/checkpoint-15500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 05:09:08,999 >> Special tokens file saved in /kaggle/workspace/output/tst-summarization/checkpoint-15500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 05:09:09,000 >> Copy vocab file to /kaggle/workspace/output/tst-summarization/checkpoint-15500/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 15%|████▌                          | 15991/107670 [2:00:15<11:38:53,  2.19it/s]","output_type":"stream"},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":" 16%|████▊                          | 16739/107670 [2:05:58<11:23:42,  2.22it/s]^C\nTraceback (most recent call last):\n  File \"/kaggle/working/transformers/examples/pytorch/summarization/run_summarization.py\", line 773, in <module>\n    main()\n  File \"/kaggle/working/transformers/examples/pytorch/summarization/run_summarization.py\", line 692, in main\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2021, in train\n    return inner_training_loop(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2357, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3487, in training_step\n    self.accelerator.backward(loss, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2159, in backward\n    loss.backward(**kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 521, in backward\n    torch.autograd.backward(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 289, in backward\n    _engine_run_backward(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py\", line 768, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"markdown","source":" # 使用混合精度进行 分布式训练","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/transformers/examples","metadata":{"execution":{"iopub.status.busy":"2024-09-19T05:19:05.006052Z","iopub.execute_input":"2024-09-19T05:19:05.006444Z","iopub.status.idle":"2024-09-19T05:19:05.013397Z","shell.execute_reply.started":"2024-09-19T05:19:05.006402Z","shell.execute_reply":"2024-09-19T05:19:05.012526Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"/kaggle/working/transformers/examples\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 检查当前环境中的GPU资源","metadata":{}},{"cell_type":"code","source":"import torch  \n# import transformers  \n# from transformers import T5Tokenizer, T5ForConditionalGeneration  \n\n# 检查GPU数量  \ngpu_count = torch.cuda.device_count()  \nprint(f\"Available GPU count: {gpu_count}\")  \n\n\n# 获取当前设备的编号  \ncurrent_device = torch.cuda.current_device()  \nprint(f\"Current CUDA device index: {current_device}\")  \n\n# 列出所有设备的名称和编号  \nfor i in range(gpu_count):  \n    print(f\"Device {i}: {torch.cuda.get_device_name(i)}\") \n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T05:25:51.148404Z","iopub.execute_input":"2024-09-19T05:25:51.149456Z","iopub.status.idle":"2024-09-19T05:25:51.201282Z","shell.execute_reply.started":"2024-09-19T05:25:51.149411Z","shell.execute_reply":"2024-09-19T05:25:51.200260Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Available GPU count: 2\nCurrent CUDA device index: 0\nDevice 0: Tesla T4\nDevice 1: Tesla T4\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install accelerate==0.34.0","metadata":{"execution":{"iopub.status.busy":"2024-09-19T05:37:33.835412Z","iopub.execute_input":"2024-09-19T05:37:33.836218Z","iopub.status.idle":"2024-09-19T05:37:49.050688Z","shell.execute_reply.started":"2024-09-19T05:37:33.836173Z","shell.execute_reply":"2024-09-19T05:37:49.049404Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Collecting accelerate==0.34.0\n  Downloading accelerate-0.34.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.34.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.34.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.34.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.34.0) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.34.0) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.34.0) (0.24.6)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.34.0) (0.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.34.0) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.34.0) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.34.0) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.34.0) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.34.0) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.34.0) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.34.0) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.34.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.34.0) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.34.0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.34.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.34.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.34.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.34.0) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.34.0) (1.3.0)\nDownloading accelerate-0.34.0-py3-none-any.whl (324 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.3/324.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.33.0\n    Uninstalling accelerate-0.33.0:\n      Successfully uninstalled accelerate-0.33.0\nSuccessfully installed accelerate-0.34.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# 这是用于多GPU分布式训练的PyTorch包  # torchrun命令会自动启动多个进程，以有效利用多个GPU\n\n!torchrun \\\n    --nproc_per_node 2 pytorch/summarization/run_summarization.py \\\n    --fp16 \\\n    --model_name_or_path google-t5/t5-small \\\n    --do_train \\\n    --do_eval \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --source_prefix \"summarize: \" \\\n    --output_dir /kaggle/workspace/output/distributed_training/tst-summarization \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --overwrite_output_dir \\\n    --predict_with_generate","metadata":{"execution":{"iopub.status.busy":"2024-09-19T05:37:53.909031Z","iopub.execute_input":"2024-09-19T05:37:53.909760Z","iopub.status.idle":"2024-09-19T06:14:44.290929Z","shell.execute_reply.started":"2024-09-19T05:37:53.909714Z","shell.execute_reply":"2024-09-19T06:14:44.289856Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"W0919 05:37:56.640000 132298363385664 torch/distributed/run.py:779] \nW0919 05:37:56.640000 132298363385664 torch/distributed/run.py:779] *****************************************\nW0919 05:37:56.640000 132298363385664 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0919 05:37:56.640000 132298363385664 torch/distributed/run.py:779] *****************************************\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nOverwrite dataset info from restored data version if exists.\nLoading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\nFound cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d)\nLoading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n[INFO|configuration_utils.py:672] 2024-09-19 05:38:08,820 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n[INFO|configuration_utils.py:739] 2024-09-19 05:38:08,824 >> Model config T5Config {\n  \"_name_or_path\": \"google-t5/t5-small\",\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 512,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 6,\n  \"num_heads\": 8,\n  \"num_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"transformers_version\": \"4.45.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 05:38:08,912 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 05:38:08,912 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 05:38:08,912 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 05:38:08,912 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 05:38:08,912 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n[INFO|modeling_utils.py:3702] 2024-09-19 05:38:09,024 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\n[INFO|configuration_utils.py:1097] 2024-09-19 05:38:09,031 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\n[INFO|modeling_utils.py:4544] 2024-09-19 05:38:09,249 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\n[INFO|modeling_utils.py:4552] 2024-09-19 05:38:09,249 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google-t5/t5-small.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n[INFO|configuration_utils.py:1052] 2024-09-19 05:38:09,343 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\n[INFO|configuration_utils.py:1097] 2024-09-19 05:38:09,343 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nLoading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-0c98e5f34d664d79.arrow\nLoading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-b70318085e7e41c4.arrow\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n[INFO|trainer.py:667] 2024-09-19 05:38:10,460 >> Using auto half precision backend\n[INFO|trainer.py:2212] 2024-09-19 05:38:11,391 >> ***** Running training *****\n[INFO|trainer.py:2213] 2024-09-19 05:38:11,391 >>   Num examples = 287,113\n[INFO|trainer.py:2214] 2024-09-19 05:38:11,392 >>   Num Epochs = 3\n[INFO|trainer.py:2215] 2024-09-19 05:38:11,392 >>   Instantaneous batch size per device = 4\n[INFO|trainer.py:2218] 2024-09-19 05:38:11,392 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n[INFO|trainer.py:2219] 2024-09-19 05:38:11,392 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:2220] 2024-09-19 05:38:11,392 >>   Total optimization steps = 107,670\n[INFO|trainer.py:2221] 2024-09-19 05:38:11,393 >>   Number of trainable parameters = 60,506,624\n  0%|                                                | 0/107670 [00:00<?, ?it/s][rank1]:[W919 05:38:12.391237108 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n[rank0]:[W919 05:38:12.391415007 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n{'loss': 1.9691, 'grad_norm': 1.801904320716858, 'learning_rate': 4.976873780997492e-05, 'epoch': 0.01}\n  0%|▏                                   | 500/107670 [02:19<8:19:22,  3.58it/s][INFO|trainer.py:3674] 2024-09-19 05:40:31,147 >> Saving model checkpoint to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-500\n[INFO|configuration_utils.py:407] 2024-09-19 05:40:31,150 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 05:40:31,151 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 05:40:31,658 >> Model weights saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 05:40:31,661 >> tokenizer config file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 05:40:31,662 >> Special tokens file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 05:40:31,663 >> Copy vocab file to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-500/spiece.model\n{'loss': 1.8993, 'grad_norm': 1.7326163053512573, 'learning_rate': 4.953654685613449e-05, 'epoch': 0.03}\n  1%|▎                                  | 1000/107670 [04:39<8:14:43,  3.59it/s][INFO|trainer.py:3674] 2024-09-19 05:42:51,147 >> Saving model checkpoint to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-1000\n[INFO|configuration_utils.py:407] 2024-09-19 05:42:51,150 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-1000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 05:42:51,151 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-1000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 05:42:51,651 >> Model weights saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-1000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 05:42:51,654 >> tokenizer config file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-1000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 05:42:51,654 >> Special tokens file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-1000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 05:42:51,655 >> Copy vocab file to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-1000/spiece.model\n{'loss': 1.8984, 'grad_norm': 2.0813467502593994, 'learning_rate': 4.9304355902294046e-05, 'epoch': 0.04}\n  1%|▍                                  | 1500/107670 [06:59<8:10:07,  3.61it/s][INFO|trainer.py:3674] 2024-09-19 05:45:10,851 >> Saving model checkpoint to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-1500\n[INFO|configuration_utils.py:407] 2024-09-19 05:45:10,854 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-1500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 05:45:10,855 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-1500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 05:45:11,363 >> Model weights saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-1500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 05:45:11,366 >> tokenizer config file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-1500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 05:45:11,367 >> Special tokens file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-1500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 05:45:11,368 >> Copy vocab file to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-1500/spiece.model\n{'loss': 1.9106, 'grad_norm': 1.613473892211914, 'learning_rate': 4.9072164948453605e-05, 'epoch': 0.06}\n  2%|▋                                  | 2000/107670 [09:19<8:03:50,  3.64it/s][INFO|trainer.py:3674] 2024-09-19 05:47:30,783 >> Saving model checkpoint to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-2000\n[INFO|configuration_utils.py:407] 2024-09-19 05:47:30,785 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-2000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 05:47:30,786 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-2000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 05:47:31,298 >> Model weights saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-2000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 05:47:31,301 >> tokenizer config file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-2000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 05:47:31,301 >> Special tokens file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-2000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 05:47:31,302 >> Copy vocab file to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-2000/spiece.model\n{'loss': 1.8969, 'grad_norm': 1.8459192514419556, 'learning_rate': 4.883997399461317e-05, 'epoch': 0.07}\n  2%|▊                                  | 2500/107670 [11:39<8:06:35,  3.60it/s][INFO|trainer.py:3674] 2024-09-19 05:49:50,940 >> Saving model checkpoint to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-2500\n[INFO|configuration_utils.py:407] 2024-09-19 05:49:50,943 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-2500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 05:49:50,943 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-2500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 05:49:51,462 >> Model weights saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-2500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 05:49:51,465 >> tokenizer config file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-2500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 05:49:51,465 >> Special tokens file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-2500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 05:49:51,466 >> Copy vocab file to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-2500/spiece.model\n{'loss': 1.8628, 'grad_norm': 1.6831793785095215, 'learning_rate': 4.8607783040772736e-05, 'epoch': 0.08}\n  3%|▉                                  | 3000/107670 [13:59<8:09:51,  3.56it/s][INFO|trainer.py:3674] 2024-09-19 05:52:10,527 >> Saving model checkpoint to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-3000\n[INFO|configuration_utils.py:407] 2024-09-19 05:52:10,530 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-3000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 05:52:10,531 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-3000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 05:52:11,196 >> Model weights saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-3000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 05:52:11,200 >> tokenizer config file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-3000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 05:52:11,201 >> Special tokens file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-3000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 05:52:11,202 >> Copy vocab file to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-3000/spiece.model\n{'loss': 1.872, 'grad_norm': 2.1099791526794434, 'learning_rate': 4.8375592086932295e-05, 'epoch': 0.1}\n  3%|█▏                                 | 3500/107670 [16:19<8:06:15,  3.57it/s][INFO|trainer.py:3674] 2024-09-19 05:54:30,448 >> Saving model checkpoint to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-3500\n[INFO|configuration_utils.py:407] 2024-09-19 05:54:30,451 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-3500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 05:54:30,451 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-3500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 05:54:30,995 >> Model weights saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-3500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 05:54:30,998 >> tokenizer config file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-3500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 05:54:30,998 >> Special tokens file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-3500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 05:54:30,999 >> Copy vocab file to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-3500/spiece.model\n{'loss': 1.8819, 'grad_norm': 1.670157551765442, 'learning_rate': 4.814386551499954e-05, 'epoch': 0.11}\n  4%|█▎                                 | 4000/107670 [18:38<7:59:14,  3.61it/s][INFO|trainer.py:3674] 2024-09-19 05:56:50,082 >> Saving model checkpoint to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-4000\n[INFO|configuration_utils.py:407] 2024-09-19 05:56:50,084 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-4000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 05:56:50,085 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-4000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 05:56:50,632 >> Model weights saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-4000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 05:56:50,635 >> tokenizer config file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-4000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 05:56:50,636 >> Special tokens file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-4000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 05:56:50,637 >> Copy vocab file to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-4000/spiece.model\n{'loss': 1.8853, 'grad_norm': 1.7648800611495972, 'learning_rate': 4.79116745611591e-05, 'epoch': 0.13}\n  4%|█▍                                 | 4500/107670 [20:58<7:56:19,  3.61it/s][INFO|trainer.py:3674] 2024-09-19 05:59:09,667 >> Saving model checkpoint to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-4500\n[INFO|configuration_utils.py:407] 2024-09-19 05:59:09,670 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-4500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 05:59:09,670 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-4500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 05:59:10,199 >> Model weights saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-4500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 05:59:10,203 >> tokenizer config file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-4500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 05:59:10,204 >> Special tokens file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-4500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 05:59:10,206 >> Copy vocab file to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-4500/spiece.model\n{'loss': 1.8754, 'grad_norm': 1.9109944105148315, 'learning_rate': 4.767948360731866e-05, 'epoch': 0.14}\n  5%|█▋                                 | 5000/107670 [23:17<7:55:39,  3.60it/s][INFO|trainer.py:3674] 2024-09-19 06:01:29,162 >> Saving model checkpoint to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-5000\n[INFO|configuration_utils.py:407] 2024-09-19 06:01:29,164 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-5000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 06:01:29,165 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-5000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 06:01:29,692 >> Model weights saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-5000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 06:01:29,695 >> tokenizer config file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-5000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 06:01:29,695 >> Special tokens file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-5000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 06:01:29,696 >> Copy vocab file to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-5000/spiece.model\n{'loss': 1.8741, 'grad_norm': 1.7853020429611206, 'learning_rate': 4.7447757035385905e-05, 'epoch': 0.15}\n  5%|█▊                                 | 5500/107670 [25:37<7:52:58,  3.60it/s][INFO|trainer.py:3674] 2024-09-19 06:03:48,499 >> Saving model checkpoint to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-5500\n[INFO|configuration_utils.py:407] 2024-09-19 06:03:48,502 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-5500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 06:03:48,503 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-5500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 06:03:49,039 >> Model weights saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-5500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 06:03:49,042 >> tokenizer config file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-5500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 06:03:49,042 >> Special tokens file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-5500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 06:03:49,043 >> Copy vocab file to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-5500/spiece.model\n{'loss': 1.876, 'grad_norm': 2.1416571140289307, 'learning_rate': 4.7215566081545464e-05, 'epoch': 0.17}\n  6%|█▉                                 | 6000/107670 [27:57<7:50:24,  3.60it/s][INFO|trainer.py:3674] 2024-09-19 06:06:08,468 >> Saving model checkpoint to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-6000\n[INFO|configuration_utils.py:407] 2024-09-19 06:06:08,471 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-6000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 06:06:08,471 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-6000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 06:06:09,011 >> Model weights saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-6000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 06:06:09,014 >> tokenizer config file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-6000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 06:06:09,015 >> Special tokens file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-6000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 06:06:09,016 >> Copy vocab file to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-6000/spiece.model\n{'loss': 1.8577, 'grad_norm': 1.592714786529541, 'learning_rate': 4.698337512770502e-05, 'epoch': 0.18}\n  6%|██                                 | 6500/107670 [30:16<7:48:27,  3.60it/s][INFO|trainer.py:3674] 2024-09-19 06:08:28,328 >> Saving model checkpoint to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-6500\n[INFO|configuration_utils.py:407] 2024-09-19 06:08:28,331 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-6500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 06:08:28,331 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-6500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 06:08:28,862 >> Model weights saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-6500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 06:08:28,865 >> tokenizer config file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-6500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 06:08:28,865 >> Special tokens file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-6500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 06:08:28,866 >> Copy vocab file to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-6500/spiece.model\n{'loss': 1.8644, 'grad_norm': 1.6999664306640625, 'learning_rate': 4.675118417386459e-05, 'epoch': 0.2}\n  7%|██▎                                | 7000/107670 [32:36<7:51:13,  3.56it/s][INFO|trainer.py:3674] 2024-09-19 06:10:48,179 >> Saving model checkpoint to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-7000\n[INFO|configuration_utils.py:407] 2024-09-19 06:10:48,182 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-7000/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 06:10:48,183 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-7000/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 06:10:48,752 >> Model weights saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-7000/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 06:10:48,755 >> tokenizer config file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-7000/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 06:10:48,756 >> Special tokens file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-7000/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 06:10:48,757 >> Copy vocab file to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-7000/spiece.model\n{'loss': 1.8678, 'grad_norm': 1.7126801013946533, 'learning_rate': 4.651899322002415e-05, 'epoch': 0.21}\n  7%|██▍                                | 7500/107670 [34:56<7:44:57,  3.59it/s][INFO|trainer.py:3674] 2024-09-19 06:13:07,851 >> Saving model checkpoint to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-7500\n[INFO|configuration_utils.py:407] 2024-09-19 06:13:07,854 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-7500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 06:13:07,854 >> Configuration saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-7500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 06:13:08,377 >> Model weights saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-7500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 06:13:08,379 >> tokenizer config file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-7500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 06:13:08,380 >> Special tokens file saved in /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-7500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 06:13:08,381 >> Copy vocab file to /kaggle/workspace/output/distributed_training/tst-summarization/checkpoint-7500/spiece.model\n  7%|██▌                                | 7841/107670 [36:31<7:39:13,  3.62it/s]^C\nW0919 06:14:43.584000 132298363385664 torch/distributed/elastic/agent/server/api.py:688] Received Signals.SIGINT death signal, shutting down workers\nW0919 06:14:43.585000 132298363385664 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 34380 closing signal SIGINT\nW0919 06:14:43.585000 132298363385664 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 34381 closing signal SIGINT\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/kaggle/working/transformers/examples/pytorch/summarization/run_summarization.py\", line 773, in <module>\n[rank1]:     main()\n[rank1]:   File \"/kaggle/working/transformers/examples/pytorch/summarization/run_summarization.py\", line 692, in main\n[rank1]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)\n[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2021, in train\n[rank1]:     return inner_training_loop(\n[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2421, in _inner_training_loop\n[rank1]:     self.optimizer.step()\n[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/accelerate/optimizer.py\", line 160, in step\n[rank1]:     self.scaler.update()\n[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py\", line 460, in update\n[rank1]:     def update(self, new_scale: Optional[Union[float, torch.Tensor]] = None) -> None:\n[rank1]: KeyboardInterrupt\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/kaggle/working/transformers/examples/pytorch/summarization/run_summarization.py\", line 773, in <module>\n[rank0]:     main()\n[rank0]:   File \"/kaggle/working/transformers/examples/pytorch/summarization/run_summarization.py\", line 692, in main\n[rank0]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)\n[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2021, in train\n[rank0]:     return inner_training_loop(\n[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2421, in _inner_training_loop\n[rank0]:     self.optimizer.step()\n[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/accelerate/optimizer.py\", line 159, in step\n[rank0]:     self.scaler.step(self.optimizer, closure)\n[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py\", line 454, in step\n[rank0]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py\", line 352, in _maybe_opt_step\n[rank0]:     retval = optimizer.step(*args, **kwargs)\n[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/accelerate/optimizer.py\", line 214, in patched_step\n[rank0]:     return method(*args, **kwargs)\n[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 130, in wrapper\n[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)\n[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 469, in wrapper\n[rank0]:     with torch.autograd.profiler.record_function(profile_name):\n[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/profiler.py\", line 705, in __exit__\n[rank0]:     torch.ops.profiler._record_function_exit._RecordFunction(record)\n[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_ops.py\", line 903, in __call__\n[rank0]:     return self_._op(*args, **kwargs)\n[rank0]: KeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 使用Accelerate库运行脚本","metadata":{}},{"cell_type":"markdown","source":"## 之前的脚本训练的本质是什么？---- Trainer类","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport evaluate\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\n\n\n# 加载数据集\ndataset = load_dataset(\"yelp_review_full\")\n# dataset[\"train\"][100]\n\n\n# 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n\n\n# 对数据集进行逐batch分词的映射函数\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n\n# 提取小一点的数据集，加速训练\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n\n\n\n# 加载模型\nmodel = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n\n\n\n\n\ntraining_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\")\n\n\n# 加载性能评估对象\nmetric = evaluate.load(\"accuracy\")\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n# 配置训练对象\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T06:18:12.562147Z","iopub.execute_input":"2024-09-19T06:18:12.563032Z","iopub.status.idle":"2024-09-19T06:27:47.348350Z","shell.execute_reply.started":"2024-09-19T06:18:12.562992Z","shell.execute_reply":"2024-09-19T06:27:47.347449Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/650000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8875795ccc794cd79e82c8b8ff02b40c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4146d75e5df24c73a0ecb8c956b1f2d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e9ca44cdcfa43c3ba1cf58208cfd793"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8a3a99ebc9f425ba365e5e4b215cc90"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [189/189 03:32, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.301590</td>\n      <td>0.415000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.966652</td>\n      <td>0.584000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.981361</td>\n      <td>0.588000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=189, training_loss=1.097913873258722, metrics={'train_runtime': 214.4263, 'train_samples_per_second': 13.991, 'train_steps_per_second': 0.881, 'total_flos': 789354427392000.0, 'train_loss': 1.097913873258722, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"###  Accelerate 是一个仅限 PyTorch 的分布式训练库\n#### 它提供了一种统一的方法，用于在多种类型的算力配置（仅限 CPU、多个 GPU、TPU）上训练模型，同时保持对 PyTorch 训练循环（epoch）的完全可见性\n\n- 它与之前方法的显著区别是，它不会调用trainer类进行训练\n- acclelerate 在库中所对应的脚本名称，与使用trainer类的脚本略有不同\n- 🤗 Accelerate 支持的脚本将在文件夹中有一个 task_no_trainer.py 文件。","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!pip install git+https://github.com/huggingface/accelerate","metadata":{"execution":{"iopub.status.busy":"2024-09-19T06:37:35.418878Z","iopub.execute_input":"2024-09-19T06:37:35.419313Z","iopub.status.idle":"2024-09-19T06:38:03.671854Z","shell.execute_reply.started":"2024-09-19T06:37:35.419270Z","shell.execute_reply":"2024-09-19T06:38:03.670665Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting git+https://github.com/huggingface/accelerate\n  Cloning https://github.com/huggingface/accelerate to /tmp/pip-req-build-dp06i5lj\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate /tmp/pip-req-build-dp06i5lj\n  Resolved https://github.com/huggingface/accelerate to commit 4305033f8035defad0a87cd38e5c918e78510ba5\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.35.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.35.0.dev0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.35.0.dev0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.35.0.dev0) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.35.0.dev0) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.35.0.dev0) (0.24.6)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.35.0.dev0) (0.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.35.0.dev0) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.35.0.dev0) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.35.0.dev0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.35.0.dev0) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.35.0.dev0) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"%ls -al /kaggle/working\n# 创建并保存配置文件\n!accelerate config default","metadata":{"execution":{"iopub.status.busy":"2024-09-19T06:39:36.677407Z","iopub.execute_input":"2024-09-19T06:39:36.677877Z","iopub.status.idle":"2024-09-19T06:39:43.555203Z","shell.execute_reply.started":"2024-09-19T06:39:36.677833Z","shell.execute_reply":"2024-09-19T06:39:43.553922Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"total 16\ndrwxr-xr-x  4 root root 4096 Sep 19 02:26 \u001b[0m\u001b[01;34m.\u001b[0m/\ndrwxr-xr-x  6 root root 4096 Sep 19 02:53 \u001b[01;34m..\u001b[0m/\ndrwxr-xr-x  2 root root 4096 Sep 19 02:23 \u001b[01;34m.virtual_documents\u001b[0m/\ndrwxr-xr-x 18 root root 4096 Sep 19 02:26 \u001b[01;34mtransformers\u001b[0m/\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n","output_type":"stream"}]},{"cell_type":"code","source":"!accelerate test","metadata":{"execution":{"iopub.status.busy":"2024-09-19T06:40:09.517306Z","iopub.execute_input":"2024-09-19T06:40:09.517824Z","iopub.status.idle":"2024-09-19T06:40:25.422060Z","shell.execute_reply.started":"2024-09-19T06:40:09.517781Z","shell.execute_reply":"2024-09-19T06:40:25.420817Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\nRunning:  accelerate-launch /opt/conda/lib/python3.10/site-packages/accelerate/test_utils/scripts/test_script.py\nstdout: **Initialization**\nstdout: Testing, testing. 1, 2, 3.\nstdout: Distributed environment: MULTI_GPU  Backend: nccl\nstdout: Num processes: 2\nstdout: Process index: 0\nstdout: Local process index: 0\nstdout: Device: cuda:0\nstdout: \nstdout: Mixed precision type: no\nstdout: \nstdout: Distributed environment: MULTI_GPU  Backend: nccl\nstdout: Num processes: 2\nstdout: Process index: 1\nstdout: Local process index: 1\nstdout: Device: cuda:1\nstdout: \nstdout: Mixed precision type: no\nstdout: \nstdout: \nstdout: **Test process execution**\nstdout: \nstdout: **Test split between processes as a list**\nstdout: \nstdout: **Test split between processes as a dict**\nstdout: \nstdout: **Test split between processes as a tensor**\nstdout: \nstdout: **Test split between processes evenly**\nstdout: \nstdout: **Test split between processes as a datasets.Dataset**\nstdout: \nstdout: **Test random number generator synchronization**\nstdout: All rng are properly synched.\nstdout: \nstdout: **DataLoader integration test**\nstdout: 1 0 tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\nstdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\nstdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\nstdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63], device='cuda:0') <class 'accelerate.data_loader.DataLoaderShard'>\nstdout: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\nstdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\nstdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\nstdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63], device='cuda:1') <class 'accelerate.data_loader.DataLoaderShard'>\nstdout: Non-shuffled dataloader passing.\nstdout: Shuffled dataloader passing.\nstdout: Non-shuffled central dataloader passing.\nstdout: Shuffled central dataloader passing.\nstdout: \nstdout: **Training integration test**\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Training yielded the same results on one CPU or distributed setup with no batch split.\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Training yielded the same results on one CPU or distributes setup with batch split.\nstdout: FP16 training check.\nstdout: FP16 training check.\nstderr: /opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:500: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\nstderr:   self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nstderr: /opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:500: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\nstderr:   self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Keep fp32 wrapper check.\nstdout: Keep fp32 wrapper check.\nstdout: BF16 training check.\nstdout: BF16 training check.\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: \nstdout: Training yielded the same results on one CPU or distributed setup with no batch split.\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: FP16 training check.\nstdout: Training yielded the same results on one CPU or distributes setup with batch split.\nstdout: FP16 training check.\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Keep fp32 wrapper check.\nstdout: Keep fp32 wrapper check.\nstdout: BF16 training check.\nstdout: BF16 training check.\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: \nstdout: **Breakpoint trigger test**\nstdout: \nstdout: **Test reinstantiated state**\nTest is a success! You are ready for your distributed training!\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/transformers/examples/pytorch/summarization/","metadata":{"execution":{"iopub.status.busy":"2024-09-19T06:44:18.888743Z","iopub.execute_input":"2024-09-19T06:44:18.889455Z","iopub.status.idle":"2024-09-19T06:44:18.896166Z","shell.execute_reply.started":"2024-09-19T06:44:18.889409Z","shell.execute_reply":"2024-09-19T06:44:18.895243Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"/kaggle/working/transformers/examples/pytorch/summarization\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 文本摘要任务可以使用的模型\n- malmarjeh/mbert2mbert-arabic-text-summarization\n- suriya7/bart-finetuned-text-summarization\n- google-t5/t5-small","metadata":{}},{"cell_type":"markdown","source":"## 使用accelerate启动训练","metadata":{}},{"cell_type":"markdown","source":"### 数据集\n11k条数据的：`autoevaluate/autoeval-staging-eval-project-cnn_dailymail-899c0b5b-10935468`","metadata":{}},{"cell_type":"code","source":"# accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 {script_name.py} {--arg1} {--arg2} ...\n\n!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 run_summarization_no_trainer.py \\\n    --model_name_or_path malmarjeh/mbert2mbert-arabic-text-summarization \\\n    --dataset_name autoevaluate/autoeval-staging-eval-project-cnn_dailymail-899c0b5b-10935468 \\\n    --dataset_config \"default\" \\\n    --source_prefix \"summarize: \" \\\n    --output_dir /kaggle/workspace/output_accelerate/tst-summarization","metadata":{"execution":{"iopub.status.busy":"2024-09-19T07:47:26.574078Z","iopub.execute_input":"2024-09-19T07:47:26.574524Z","iopub.status.idle":"2024-09-19T07:48:29.752557Z","shell.execute_reply.started":"2024-09-19T07:47:26.574483Z","shell.execute_reply":"2024-09-19T07:48:29.751337Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:500: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:500: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nDownloading data: 100%|████████████████████| 35.6M/35.6M [00:01<00:00, 20.6MB/s]\nGenerating train split: 100%|███| 11490/11490 [00:00<00:00, 16124.79 examples/s]\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--malmarjeh--mbert2mbert-arabic-text-summarization/snapshots/3b2e7db2977b89b818b35d9eae4fcd6244ef2c40/config.json\nModel config EncoderDecoderConfig {\n  \"_name_or_path\": \"malmarjeh/mbert2mbert-arabic-text-summarization\",\n  \"architectures\": [\n    \"EncoderDecoderModel\"\n  ],\n  \"decoder\": {\n    \"_name_or_path\": \"bert-base-multilingual-cased\",\n    \"add_cross_attention\": true,\n    \"architectures\": [\n      \"BertForMaskedLM\"\n    ],\n    \"attention_probs_dropout_prob\": 0.1,\n    \"bad_words_ids\": null,\n    \"begin_suppress_tokens\": null,\n    \"bos_token_id\": null,\n    \"chunk_size_feed_forward\": 0,\n    \"classifier_dropout\": null,\n    \"cross_attention_hidden_size\": null,\n    \"decoder_start_token_id\": null,\n    \"directionality\": \"bidi\",\n    \"diversity_penalty\": 0.0,\n    \"do_sample\": false,\n    \"early_stopping\": false,\n    \"encoder_no_repeat_ngram_size\": 0,\n    \"eos_token_id\": null,\n    \"exponential_decay_length_penalty\": null,\n    \"finetuning_task\": null,\n    \"forced_bos_token_id\": null,\n    \"forced_eos_token_id\": null,\n    \"gradient_checkpointing\": false,\n    \"hidden_act\": \"gelu\",\n    \"hidden_dropout_prob\": 0.1,\n    \"hidden_size\": 768,\n    \"id2label\": {\n      \"0\": \"LABEL_0\",\n      \"1\": \"LABEL_1\"\n    },\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 3072,\n    \"is_decoder\": true,\n    \"is_encoder_decoder\": false,\n    \"label2id\": {\n      \"LABEL_0\": 0,\n      \"LABEL_1\": 1\n    },\n    \"layer_norm_eps\": 1e-12,\n    \"length_penalty\": 1.0,\n    \"max_length\": 20,\n    \"max_position_embeddings\": 512,\n    \"min_length\": 0,\n    \"model_type\": \"bert\",\n    \"no_repeat_ngram_size\": 0,\n    \"num_attention_heads\": 12,\n    \"num_beam_groups\": 1,\n    \"num_beams\": 1,\n    \"num_hidden_layers\": 12,\n    \"num_return_sequences\": 1,\n    \"output_attentions\": false,\n    \"output_hidden_states\": false,\n    \"output_scores\": false,\n    \"pad_token_id\": 0,\n    \"pooler_fc_size\": 768,\n    \"pooler_num_attention_heads\": 12,\n    \"pooler_num_fc_layers\": 3,\n    \"pooler_size_per_head\": 128,\n    \"pooler_type\": \"first_token_transform\",\n    \"position_embedding_type\": \"absolute\",\n    \"prefix\": null,\n    \"problem_type\": null,\n    \"pruned_heads\": {},\n    \"remove_invalid_values\": false,\n    \"repetition_penalty\": 1.0,\n    \"return_dict\": true,\n    \"return_dict_in_generate\": false,\n    \"sep_token_id\": null,\n    \"suppress_tokens\": null,\n    \"task_specific_params\": null,\n    \"temperature\": 1.0,\n    \"tf_legacy_loss\": false,\n    \"tie_encoder_decoder\": false,\n    \"tie_word_embeddings\": true,\n    \"tokenizer_class\": null,\n    \"top_k\": 50,\n    \"top_p\": 1.0,\n    \"torch_dtype\": null,\n    \"torchscript\": false,\n    \"type_vocab_size\": 2,\n    \"typical_p\": 1.0,\n    \"use_bfloat16\": false,\n    \"use_cache\": true,\n    \"vocab_size\": 119547\n  },\n  \"decoder_start_token_id\": 101,\n  \"encoder\": {\n    \"_name_or_path\": \"bert-base-multilingual-cased\",\n    \"add_cross_attention\": false,\n    \"architectures\": [\n      \"BertForMaskedLM\"\n    ],\n    \"attention_probs_dropout_prob\": 0.1,\n    \"bad_words_ids\": null,\n    \"begin_suppress_tokens\": null,\n    \"bos_token_id\": null,\n    \"chunk_size_feed_forward\": 0,\n    \"classifier_dropout\": null,\n    \"cross_attention_hidden_size\": null,\n    \"decoder_start_token_id\": null,\n    \"directionality\": \"bidi\",\n    \"diversity_penalty\": 0.0,\n    \"do_sample\": false,\n    \"early_stopping\": false,\n    \"encoder_no_repeat_ngram_size\": 0,\n    \"eos_token_id\": null,\n    \"exponential_decay_length_penalty\": null,\n    \"finetuning_task\": null,\n    \"forced_bos_token_id\": null,\n    \"forced_eos_token_id\": null,\n    \"gradient_checkpointing\": false,\n    \"hidden_act\": \"gelu\",\n    \"hidden_dropout_prob\": 0.1,\n    \"hidden_size\": 768,\n    \"id2label\": {\n      \"0\": \"LABEL_0\",\n      \"1\": \"LABEL_1\"\n    },\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 3072,\n    \"is_decoder\": false,\n    \"is_encoder_decoder\": false,\n    \"label2id\": {\n      \"LABEL_0\": 0,\n      \"LABEL_1\": 1\n    },\n    \"layer_norm_eps\": 1e-12,\n    \"length_penalty\": 1.0,\n    \"max_length\": 20,\n    \"max_position_embeddings\": 512,\n    \"min_length\": 0,\n    \"model_type\": \"bert\",\n    \"no_repeat_ngram_size\": 0,\n    \"num_attention_heads\": 12,\n    \"num_beam_groups\": 1,\n    \"num_beams\": 1,\n    \"num_hidden_layers\": 12,\n    \"num_return_sequences\": 1,\n    \"output_attentions\": false,\n    \"output_hidden_states\": false,\n    \"output_scores\": false,\n    \"pad_token_id\": 0,\n    \"pooler_fc_size\": 768,\n    \"pooler_num_attention_heads\": 12,\n    \"pooler_num_fc_layers\": 3,\n    \"pooler_size_per_head\": 128,\n    \"pooler_type\": \"first_token_transform\",\n    \"position_embedding_type\": \"absolute\",\n    \"prefix\": null,\n    \"problem_type\": null,\n    \"pruned_heads\": {},\n    \"remove_invalid_values\": false,\n    \"repetition_penalty\": 1.0,\n    \"return_dict\": true,\n    \"return_dict_in_generate\": false,\n    \"sep_token_id\": null,\n    \"suppress_tokens\": null,\n    \"task_specific_params\": null,\n    \"temperature\": 1.0,\n    \"tf_legacy_loss\": false,\n    \"tie_encoder_decoder\": false,\n    \"tie_word_embeddings\": true,\n    \"tokenizer_class\": null,\n    \"top_k\": 50,\n    \"top_p\": 1.0,\n    \"torch_dtype\": null,\n    \"torchscript\": false,\n    \"type_vocab_size\": 2,\n    \"typical_p\": 1.0,\n    \"use_bfloat16\": false,\n    \"use_cache\": true,\n    \"vocab_size\": 119547\n  },\n  \"eos_token_id\": 102,\n  \"is_encoder_decoder\": true,\n  \"max_length\": 42,\n  \"min_length\": 5,\n  \"model_type\": \"encoder-decoder\",\n  \"pad_token_id\": 0,\n  \"tie_encoder_decoder\": true,\n  \"transformers_version\": \"4.45.0.dev0\",\n  \"vocab_size\": 119547\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--malmarjeh--mbert2mbert-arabic-text-summarization/snapshots/3b2e7db2977b89b818b35d9eae4fcd6244ef2c40/config.json\nModel config EncoderDecoderConfig {\n  \"_name_or_path\": \"malmarjeh/mbert2mbert-arabic-text-summarization\",\n  \"architectures\": [\n    \"EncoderDecoderModel\"\n  ],\n  \"decoder\": {\n    \"_name_or_path\": \"bert-base-multilingual-cased\",\n    \"add_cross_attention\": true,\n    \"architectures\": [\n      \"BertForMaskedLM\"\n    ],\n    \"attention_probs_dropout_prob\": 0.1,\n    \"bad_words_ids\": null,\n    \"begin_suppress_tokens\": null,\n    \"bos_token_id\": null,\n    \"chunk_size_feed_forward\": 0,\n    \"classifier_dropout\": null,\n    \"cross_attention_hidden_size\": null,\n    \"decoder_start_token_id\": null,\n    \"directionality\": \"bidi\",\n    \"diversity_penalty\": 0.0,\n    \"do_sample\": false,\n    \"early_stopping\": false,\n    \"encoder_no_repeat_ngram_size\": 0,\n    \"eos_token_id\": null,\n    \"exponential_decay_length_penalty\": null,\n    \"finetuning_task\": null,\n    \"forced_bos_token_id\": null,\n    \"forced_eos_token_id\": null,\n    \"gradient_checkpointing\": false,\n    \"hidden_act\": \"gelu\",\n    \"hidden_dropout_prob\": 0.1,\n    \"hidden_size\": 768,\n    \"id2label\": {\n      \"0\": \"LABEL_0\",\n      \"1\": \"LABEL_1\"\n    },\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 3072,\n    \"is_decoder\": true,\n    \"is_encoder_decoder\": false,\n    \"label2id\": {\n      \"LABEL_0\": 0,\n      \"LABEL_1\": 1\n    },\n    \"layer_norm_eps\": 1e-12,\n    \"length_penalty\": 1.0,\n    \"max_length\": 20,\n    \"max_position_embeddings\": 512,\n    \"min_length\": 0,\n    \"model_type\": \"bert\",\n    \"no_repeat_ngram_size\": 0,\n    \"num_attention_heads\": 12,\n    \"num_beam_groups\": 1,\n    \"num_beams\": 1,\n    \"num_hidden_layers\": 12,\n    \"num_return_sequences\": 1,\n    \"output_attentions\": false,\n    \"output_hidden_states\": false,\n    \"output_scores\": false,\n    \"pad_token_id\": 0,\n    \"pooler_fc_size\": 768,\n    \"pooler_num_attention_heads\": 12,\n    \"pooler_num_fc_layers\": 3,\n    \"pooler_size_per_head\": 128,\n    \"pooler_type\": \"first_token_transform\",\n    \"position_embedding_type\": \"absolute\",\n    \"prefix\": null,\n    \"problem_type\": null,\n    \"pruned_heads\": {},\n    \"remove_invalid_values\": false,\n    \"repetition_penalty\": 1.0,\n    \"return_dict\": true,\n    \"return_dict_in_generate\": false,\n    \"sep_token_id\": null,\n    \"suppress_tokens\": null,\n    \"task_specific_params\": null,\n    \"temperature\": 1.0,\n    \"tf_legacy_loss\": false,\n    \"tie_encoder_decoder\": false,\n    \"tie_word_embeddings\": true,\n    \"tokenizer_class\": null,\n    \"top_k\": 50,\n    \"top_p\": 1.0,\n    \"torch_dtype\": null,\n    \"torchscript\": false,\n    \"type_vocab_size\": 2,\n    \"typical_p\": 1.0,\n    \"use_bfloat16\": false,\n    \"use_cache\": true,\n    \"vocab_size\": 119547\n  },\n  \"decoder_start_token_id\": 101,\n  \"encoder\": {\n    \"_name_or_path\": \"bert-base-multilingual-cased\",\n    \"add_cross_attention\": false,\n    \"architectures\": [\n      \"BertForMaskedLM\"\n    ],\n    \"attention_probs_dropout_prob\": 0.1,\n    \"bad_words_ids\": null,\n    \"begin_suppress_tokens\": null,\n    \"bos_token_id\": null,\n    \"chunk_size_feed_forward\": 0,\n    \"classifier_dropout\": null,\n    \"cross_attention_hidden_size\": null,\n    \"decoder_start_token_id\": null,\n    \"directionality\": \"bidi\",\n    \"diversity_penalty\": 0.0,\n    \"do_sample\": false,\n    \"early_stopping\": false,\n    \"encoder_no_repeat_ngram_size\": 0,\n    \"eos_token_id\": null,\n    \"exponential_decay_length_penalty\": null,\n    \"finetuning_task\": null,\n    \"forced_bos_token_id\": null,\n    \"forced_eos_token_id\": null,\n    \"gradient_checkpointing\": false,\n    \"hidden_act\": \"gelu\",\n    \"hidden_dropout_prob\": 0.1,\n    \"hidden_size\": 768,\n    \"id2label\": {\n      \"0\": \"LABEL_0\",\n      \"1\": \"LABEL_1\"\n    },\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 3072,\n    \"is_decoder\": false,\n    \"is_encoder_decoder\": false,\n    \"label2id\": {\n      \"LABEL_0\": 0,\n      \"LABEL_1\": 1\n    },\n    \"layer_norm_eps\": 1e-12,\n    \"length_penalty\": 1.0,\n    \"max_length\": 20,\n    \"max_position_embeddings\": 512,\n    \"min_length\": 0,\n    \"model_type\": \"bert\",\n    \"no_repeat_ngram_size\": 0,\n    \"num_attention_heads\": 12,\n    \"num_beam_groups\": 1,\n    \"num_beams\": 1,\n    \"num_hidden_layers\": 12,\n    \"num_return_sequences\": 1,\n    \"output_attentions\": false,\n    \"output_hidden_states\": false,\n    \"output_scores\": false,\n    \"pad_token_id\": 0,\n    \"pooler_fc_size\": 768,\n    \"pooler_num_attention_heads\": 12,\n    \"pooler_num_fc_layers\": 3,\n    \"pooler_size_per_head\": 128,\n    \"pooler_type\": \"first_token_transform\",\n    \"position_embedding_type\": \"absolute\",\n    \"prefix\": null,\n    \"problem_type\": null,\n    \"pruned_heads\": {},\n    \"remove_invalid_values\": false,\n    \"repetition_penalty\": 1.0,\n    \"return_dict\": true,\n    \"return_dict_in_generate\": false,\n    \"sep_token_id\": null,\n    \"suppress_tokens\": null,\n    \"task_specific_params\": null,\n    \"temperature\": 1.0,\n    \"tf_legacy_loss\": false,\n    \"tie_encoder_decoder\": false,\n    \"tie_word_embeddings\": true,\n    \"tokenizer_class\": null,\n    \"top_k\": 50,\n    \"top_p\": 1.0,\n    \"torch_dtype\": null,\n    \"torchscript\": false,\n    \"type_vocab_size\": 2,\n    \"typical_p\": 1.0,\n    \"use_bfloat16\": false,\n    \"use_cache\": true,\n    \"vocab_size\": 119547\n  },\n  \"eos_token_id\": 102,\n  \"is_encoder_decoder\": true,\n  \"max_length\": 42,\n  \"min_length\": 5,\n  \"model_type\": \"encoder-decoder\",\n  \"pad_token_id\": 0,\n  \"tie_encoder_decoder\": true,\n  \"transformers_version\": \"4.45.0.dev0\",\n  \"vocab_size\": 119547\n}\n\nloading file vocab.txt from cache at /root/.cache/huggingface/hub/models--malmarjeh--mbert2mbert-arabic-text-summarization/snapshots/3b2e7db2977b89b818b35d9eae4fcd6244ef2c40/vocab.txt\nloading file tokenizer.json from cache at None\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--malmarjeh--mbert2mbert-arabic-text-summarization/snapshots/3b2e7db2977b89b818b35d9eae4fcd6244ef2c40/special_tokens_map.json\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--malmarjeh--mbert2mbert-arabic-text-summarization/snapshots/3b2e7db2977b89b818b35d9eae4fcd6244ef2c40/tokenizer_config.json\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--malmarjeh--mbert2mbert-arabic-text-summarization/snapshots/3b2e7db2977b89b818b35d9eae4fcd6244ef2c40/config.json\nModel config EncoderDecoderConfig {\n  \"_name_or_path\": \"malmarjeh/mbert2mbert-arabic-text-summarization\",\n  \"architectures\": [\n    \"EncoderDecoderModel\"\n  ],\n  \"decoder\": {\n    \"_name_or_path\": \"bert-base-multilingual-cased\",\n    \"add_cross_attention\": true,\n    \"architectures\": [\n      \"BertForMaskedLM\"\n    ],\n    \"attention_probs_dropout_prob\": 0.1,\n    \"bad_words_ids\": null,\n    \"begin_suppress_tokens\": null,\n    \"bos_token_id\": null,\n    \"chunk_size_feed_forward\": 0,\n    \"classifier_dropout\": null,\n    \"cross_attention_hidden_size\": null,\n    \"decoder_start_token_id\": null,\n    \"directionality\": \"bidi\",\n    \"diversity_penalty\": 0.0,\n    \"do_sample\": false,\n    \"early_stopping\": false,\n    \"encoder_no_repeat_ngram_size\": 0,\n    \"eos_token_id\": null,\n    \"exponential_decay_length_penalty\": null,\n    \"finetuning_task\": null,\n    \"forced_bos_token_id\": null,\n    \"forced_eos_token_id\": null,\n    \"gradient_checkpointing\": false,\n    \"hidden_act\": \"gelu\",\n    \"hidden_dropout_prob\": 0.1,\n    \"hidden_size\": 768,\n    \"id2label\": {\n      \"0\": \"LABEL_0\",\n      \"1\": \"LABEL_1\"\n    },\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 3072,\n    \"is_decoder\": true,\n    \"is_encoder_decoder\": false,\n    \"label2id\": {\n      \"LABEL_0\": 0,\n      \"LABEL_1\": 1\n    },\n    \"layer_norm_eps\": 1e-12,\n    \"length_penalty\": 1.0,\n    \"max_length\": 20,\n    \"max_position_embeddings\": 512,\n    \"min_length\": 0,\n    \"model_type\": \"bert\",\n    \"no_repeat_ngram_size\": 0,\n    \"num_attention_heads\": 12,\n    \"num_beam_groups\": 1,\n    \"num_beams\": 1,\n    \"num_hidden_layers\": 12,\n    \"num_return_sequences\": 1,\n    \"output_attentions\": false,\n    \"output_hidden_states\": false,\n    \"output_scores\": false,\n    \"pad_token_id\": 0,\n    \"pooler_fc_size\": 768,\n    \"pooler_num_attention_heads\": 12,\n    \"pooler_num_fc_layers\": 3,\n    \"pooler_size_per_head\": 128,\n    \"pooler_type\": \"first_token_transform\",\n    \"position_embedding_type\": \"absolute\",\n    \"prefix\": null,\n    \"problem_type\": null,\n    \"pruned_heads\": {},\n    \"remove_invalid_values\": false,\n    \"repetition_penalty\": 1.0,\n    \"return_dict\": true,\n    \"return_dict_in_generate\": false,\n    \"sep_token_id\": null,\n    \"suppress_tokens\": null,\n    \"task_specific_params\": null,\n    \"temperature\": 1.0,\n    \"tf_legacy_loss\": false,\n    \"tie_encoder_decoder\": false,\n    \"tie_word_embeddings\": true,\n    \"tokenizer_class\": null,\n    \"top_k\": 50,\n    \"top_p\": 1.0,\n    \"torch_dtype\": null,\n    \"torchscript\": false,\n    \"type_vocab_size\": 2,\n    \"typical_p\": 1.0,\n    \"use_bfloat16\": false,\n    \"use_cache\": true,\n    \"vocab_size\": 119547\n  },\n  \"decoder_start_token_id\": 101,\n  \"encoder\": {\n    \"_name_or_path\": \"bert-base-multilingual-cased\",\n    \"add_cross_attention\": false,\n    \"architectures\": [\n      \"BertForMaskedLM\"\n    ],\n    \"attention_probs_dropout_prob\": 0.1,\n    \"bad_words_ids\": null,\n    \"begin_suppress_tokens\": null,\n    \"bos_token_id\": null,\n    \"chunk_size_feed_forward\": 0,\n    \"classifier_dropout\": null,\n    \"cross_attention_hidden_size\": null,\n    \"decoder_start_token_id\": null,\n    \"directionality\": \"bidi\",\n    \"diversity_penalty\": 0.0,\n    \"do_sample\": false,\n    \"early_stopping\": false,\n    \"encoder_no_repeat_ngram_size\": 0,\n    \"eos_token_id\": null,\n    \"exponential_decay_length_penalty\": null,\n    \"finetuning_task\": null,\n    \"forced_bos_token_id\": null,\n    \"forced_eos_token_id\": null,\n    \"gradient_checkpointing\": false,\n    \"hidden_act\": \"gelu\",\n    \"hidden_dropout_prob\": 0.1,\n    \"hidden_size\": 768,\n    \"id2label\": {\n      \"0\": \"LABEL_0\",\n      \"1\": \"LABEL_1\"\n    },\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 3072,\n    \"is_decoder\": false,\n    \"is_encoder_decoder\": false,\n    \"label2id\": {\n      \"LABEL_0\": 0,\n      \"LABEL_1\": 1\n    },\n    \"layer_norm_eps\": 1e-12,\n    \"length_penalty\": 1.0,\n    \"max_length\": 20,\n    \"max_position_embeddings\": 512,\n    \"min_length\": 0,\n    \"model_type\": \"bert\",\n    \"no_repeat_ngram_size\": 0,\n    \"num_attention_heads\": 12,\n    \"num_beam_groups\": 1,\n    \"num_beams\": 1,\n    \"num_hidden_layers\": 12,\n    \"num_return_sequences\": 1,\n    \"output_attentions\": false,\n    \"output_hidden_states\": false,\n    \"output_scores\": false,\n    \"pad_token_id\": 0,\n    \"pooler_fc_size\": 768,\n    \"pooler_num_attention_heads\": 12,\n    \"pooler_num_fc_layers\": 3,\n    \"pooler_size_per_head\": 128,\n    \"pooler_type\": \"first_token_transform\",\n    \"position_embedding_type\": \"absolute\",\n    \"prefix\": null,\n    \"problem_type\": null,\n    \"pruned_heads\": {},\n    \"remove_invalid_values\": false,\n    \"repetition_penalty\": 1.0,\n    \"return_dict\": true,\n    \"return_dict_in_generate\": false,\n    \"sep_token_id\": null,\n    \"suppress_tokens\": null,\n    \"task_specific_params\": null,\n    \"temperature\": 1.0,\n    \"tf_legacy_loss\": false,\n    \"tie_encoder_decoder\": false,\n    \"tie_word_embeddings\": true,\n    \"tokenizer_class\": null,\n    \"top_k\": 50,\n    \"top_p\": 1.0,\n    \"torch_dtype\": null,\n    \"torchscript\": false,\n    \"type_vocab_size\": 2,\n    \"typical_p\": 1.0,\n    \"use_bfloat16\": false,\n    \"use_cache\": true,\n    \"vocab_size\": 119547\n  },\n  \"eos_token_id\": 102,\n  \"is_encoder_decoder\": true,\n  \"max_length\": 42,\n  \"min_length\": 5,\n  \"model_type\": \"encoder-decoder\",\n  \"pad_token_id\": 0,\n  \"tie_encoder_decoder\": true,\n  \"transformers_version\": \"4.45.0.dev0\",\n  \"vocab_size\": 119547\n}\n\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--malmarjeh--mbert2mbert-arabic-text-summarization/snapshots/3b2e7db2977b89b818b35d9eae4fcd6244ef2c40/config.json\nModel config EncoderDecoderConfig {\n  \"_name_or_path\": \"malmarjeh/mbert2mbert-arabic-text-summarization\",\n  \"architectures\": [\n    \"EncoderDecoderModel\"\n  ],\n  \"decoder\": {\n    \"_name_or_path\": \"bert-base-multilingual-cased\",\n    \"add_cross_attention\": true,\n    \"architectures\": [\n      \"BertForMaskedLM\"\n    ],\n    \"attention_probs_dropout_prob\": 0.1,\n    \"bad_words_ids\": null,\n    \"begin_suppress_tokens\": null,\n    \"bos_token_id\": null,\n    \"chunk_size_feed_forward\": 0,\n    \"classifier_dropout\": null,\n    \"cross_attention_hidden_size\": null,\n    \"decoder_start_token_id\": null,\n    \"directionality\": \"bidi\",\n    \"diversity_penalty\": 0.0,\n    \"do_sample\": false,\n    \"early_stopping\": false,\n    \"encoder_no_repeat_ngram_size\": 0,\n    \"eos_token_id\": null,\n    \"exponential_decay_length_penalty\": null,\n    \"finetuning_task\": null,\n    \"forced_bos_token_id\": null,\n    \"forced_eos_token_id\": null,\n    \"gradient_checkpointing\": false,\n    \"hidden_act\": \"gelu\",\n    \"hidden_dropout_prob\": 0.1,\n    \"hidden_size\": 768,\n    \"id2label\": {\n      \"0\": \"LABEL_0\",\n      \"1\": \"LABEL_1\"\n    },\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 3072,\n    \"is_decoder\": true,\n    \"is_encoder_decoder\": false,\n    \"label2id\": {\n      \"LABEL_0\": 0,\n      \"LABEL_1\": 1\n    },\n    \"layer_norm_eps\": 1e-12,\n    \"length_penalty\": 1.0,\n    \"max_length\": 20,\n    \"max_position_embeddings\": 512,\n    \"min_length\": 0,\n    \"model_type\": \"bert\",\n    \"no_repeat_ngram_size\": 0,\n    \"num_attention_heads\": 12,\n    \"num_beam_groups\": 1,\n    \"num_beams\": 1,\n    \"num_hidden_layers\": 12,\n    \"num_return_sequences\": 1,\n    \"output_attentions\": false,\n    \"output_hidden_states\": false,\n    \"output_scores\": false,\n    \"pad_token_id\": 0,\n    \"pooler_fc_size\": 768,\n    \"pooler_num_attention_heads\": 12,\n    \"pooler_num_fc_layers\": 3,\n    \"pooler_size_per_head\": 128,\n    \"pooler_type\": \"first_token_transform\",\n    \"position_embedding_type\": \"absolute\",\n    \"prefix\": null,\n    \"problem_type\": null,\n    \"pruned_heads\": {},\n    \"remove_invalid_values\": false,\n    \"repetition_penalty\": 1.0,\n    \"return_dict\": true,\n    \"return_dict_in_generate\": false,\n    \"sep_token_id\": null,\n    \"suppress_tokens\": null,\n    \"task_specific_params\": null,\n    \"temperature\": 1.0,\n    \"tf_legacy_loss\": false,\n    \"tie_encoder_decoder\": false,\n    \"tie_word_embeddings\": true,\n    \"tokenizer_class\": null,\n    \"top_k\": 50,\n    \"top_p\": 1.0,\n    \"torch_dtype\": null,\n    \"torchscript\": false,\n    \"type_vocab_size\": 2,\n    \"typical_p\": 1.0,\n    \"use_bfloat16\": false,\n    \"use_cache\": true,\n    \"vocab_size\": 119547\n  },\n  \"decoder_start_token_id\": 101,\n  \"encoder\": {\n    \"_name_or_path\": \"bert-base-multilingual-cased\",\n    \"add_cross_attention\": false,\n    \"architectures\": [\n      \"BertForMaskedLM\"\n    ],\n    \"attention_probs_dropout_prob\": 0.1,\n    \"bad_words_ids\": null,\n    \"begin_suppress_tokens\": null,\n    \"bos_token_id\": null,\n    \"chunk_size_feed_forward\": 0,\n    \"classifier_dropout\": null,\n    \"cross_attention_hidden_size\": null,\n    \"decoder_start_token_id\": null,\n    \"directionality\": \"bidi\",\n    \"diversity_penalty\": 0.0,\n    \"do_sample\": false,\n    \"early_stopping\": false,\n    \"encoder_no_repeat_ngram_size\": 0,\n    \"eos_token_id\": null,\n    \"exponential_decay_length_penalty\": null,\n    \"finetuning_task\": null,\n    \"forced_bos_token_id\": null,\n    \"forced_eos_token_id\": null,\n    \"gradient_checkpointing\": false,\n    \"hidden_act\": \"gelu\",\n    \"hidden_dropout_prob\": 0.1,\n    \"hidden_size\": 768,\n    \"id2label\": {\n      \"0\": \"LABEL_0\",\n      \"1\": \"LABEL_1\"\n    },\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 3072,\n    \"is_decoder\": false,\n    \"is_encoder_decoder\": false,\n    \"label2id\": {\n      \"LABEL_0\": 0,\n      \"LABEL_1\": 1\n    },\n    \"layer_norm_eps\": 1e-12,\n    \"length_penalty\": 1.0,\n    \"max_length\": 20,\n    \"max_position_embeddings\": 512,\n    \"min_length\": 0,\n    \"model_type\": \"bert\",\n    \"no_repeat_ngram_size\": 0,\n    \"num_attention_heads\": 12,\n    \"num_beam_groups\": 1,\n    \"num_beams\": 1,\n    \"num_hidden_layers\": 12,\n    \"num_return_sequences\": 1,\n    \"output_attentions\": false,\n    \"output_hidden_states\": false,\n    \"output_scores\": false,\n    \"pad_token_id\": 0,\n    \"pooler_fc_size\": 768,\n    \"pooler_num_attention_heads\": 12,\n    \"pooler_num_fc_layers\": 3,\n    \"pooler_size_per_head\": 128,\n    \"pooler_type\": \"first_token_transform\",\n    \"position_embedding_type\": \"absolute\",\n    \"prefix\": null,\n    \"problem_type\": null,\n    \"pruned_heads\": {},\n    \"remove_invalid_values\": false,\n    \"repetition_penalty\": 1.0,\n    \"return_dict\": true,\n    \"return_dict_in_generate\": false,\n    \"sep_token_id\": null,\n    \"suppress_tokens\": null,\n    \"task_specific_params\": null,\n    \"temperature\": 1.0,\n    \"tf_legacy_loss\": false,\n    \"tie_encoder_decoder\": false,\n    \"tie_word_embeddings\": true,\n    \"tokenizer_class\": null,\n    \"top_k\": 50,\n    \"top_p\": 1.0,\n    \"torch_dtype\": null,\n    \"torchscript\": false,\n    \"type_vocab_size\": 2,\n    \"typical_p\": 1.0,\n    \"use_bfloat16\": false,\n    \"use_cache\": true,\n    \"vocab_size\": 119547\n  },\n  \"eos_token_id\": 102,\n  \"is_encoder_decoder\": true,\n  \"max_length\": 42,\n  \"min_length\": 5,\n  \"model_type\": \"encoder-decoder\",\n  \"pad_token_id\": 0,\n  \"tie_encoder_decoder\": true,\n  \"transformers_version\": \"4.45.0.dev0\",\n  \"vocab_size\": 119547\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--malmarjeh--mbert2mbert-arabic-text-summarization/snapshots/3b2e7db2977b89b818b35d9eae4fcd6244ef2c40/pytorch_model.bin\nGenerate config GenerationConfig {\n  \"decoder_start_token_id\": 101,\n  \"eos_token_id\": 102,\n  \"max_length\": 42,\n  \"min_length\": 5,\n  \"pad_token_id\": 0\n}\n\nGenerate config GenerationConfig {\n  \"pad_token_id\": 0\n}\n\nThe following encoder weights were not tied to the decoder ['bert/pooler']\nThe following encoder weights were not tied to the decoder ['bert/pooler']\nThe following encoder weights were not tied to the decoder ['bert/pooler']\nAll model checkpoint weights were used when initializing EncoderDecoderModel.\n\nAll the weights of EncoderDecoderModel were initialized from the model checkpoint at malmarjeh/mbert2mbert-arabic-text-summarization.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\nThe following encoder weights were not tied to the decoder ['bert/pooler']\nGeneration config file not found, using a generation config created from the model config.\nRunning tokenizer on dataset: 100%|█| 11490/11490 [00:31<00:00, 359.93 examples/\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/kaggle/working/transformers/examples/pytorch/summarization/run_summarization_no_trainer.py\", line 801, in <module>\n[rank0]:     main()\n[rank0]:   File \"/kaggle/working/transformers/examples/pytorch/summarization/run_summarization_no_trainer.py\", line 523, in main\n[rank0]:     eval_dataset = raw_datasets[\"validation\"].map(\n[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py\", line 75, in __getitem__\n[rank0]:     return super().__getitem__(k)\n[rank0]: KeyError: 'validation'\n[rank0]:[W919 07:48:27.802732975 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nW0919 07:48:28.692000 135251665852224 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 37227 closing signal SIGTERM\nE0919 07:48:28.906000 135251665852224 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 37226) of binary: /opt/conda/bin/python3.10\nTraceback (most recent call last):\n  File \"/opt/conda/bin/accelerate\", line 8, in <module>\n    sys.exit(main())\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n    args.func(args)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 1159, in launch_command\n    multi_gpu_launcher(args)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 793, in multi_gpu_launcher\n    distrib_run.run(args)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in run\n    elastic_launch(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 133, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nrun_summarization_no_trainer.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-09-19_07:48:28\n  host      : 4fa5516cabac\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 37226)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 使用自定义数据集运行脚本","metadata":{}},{"cell_type":"markdown","source":"### 现有数据集的格式\n```python\n{'id': '0054d6d30dbcad772e20b22771153a2a9cbeaf62',\n 'article': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx',\n 'highlights': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'}\n```","metadata":{}},{"cell_type":"markdown","source":"### 创建训练集，我们先加载BBC数据集，再改成CNN-dailymail的格式","metadata":{}},{"cell_type":"code","source":"!pip install gensim==3.6.0","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:02:55.592829Z","iopub.execute_input":"2024-09-19T08:02:55.593629Z","iopub.status.idle":"2024-09-19T08:03:25.160279Z","shell.execute_reply.started":"2024-09-19T08:02:55.593558Z","shell.execute_reply":"2024-09-19T08:03:25.159239Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting gensim==3.6.0\n  Downloading gensim-3.6.0.tar.gz (23.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.10/site-packages (from gensim==3.6.0) (1.26.4)\nRequirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from gensim==3.6.0) (1.13.1)\nRequirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from gensim==3.6.0) (1.16.0)\nRequirement already satisfied: smart_open>=1.2.1 in /opt/conda/lib/python3.10/site-packages (from gensim==3.6.0) (7.0.4)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart_open>=1.2.1->gensim==3.6.0) (1.16.0)\nBuilding wheels for collected packages: gensim\n  Building wheel for gensim (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gensim: filename=gensim-3.6.0-cp310-cp310-linux_x86_64.whl size=23370660 sha256=1b8b66e658127d5601fce8115a865da67dd74da4def46c2c9135e470176d445a\n  Stored in directory: /root/.cache/pip/wheels/00/e8/47/96f55c3144a5ea3537f549f7a97607011f5004b9f13fa8dcc5\nSuccessfully built gensim\nInstalling collected packages: gensim\n  Attempting uninstall: gensim\n    Found existing installation: gensim 4.3.3\n    Uninstalling gensim-4.3.3:\n      Successfully uninstalled gensim-4.3.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscattertext 0.1.19 requires gensim>=4.0.0, but you have gensim 3.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed gensim-3.6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# 从huggingface下载一个新的数据集，并修改其格式\nfrom datasets import load_dataset\nimport json  \nimport random\nimport string  \n\ndataset = load_dataset(\"SetFit/bbc-news\")\n\n# 只选择前10000条数据  \ntrain_dataset = dataset['train'].select(range(900)) \ntest_dataset = dataset['test'].select(range(100)) \n\n# 随机生成长度为20的字符串id\ndef generate_random_string(length=20):  \n    # Choose from all lowercase letters and digits  \n    characters = string.ascii_letters + string.digits  \n    # Use random.choice to select a character from the characters  \n    return ''.join(random.choice(characters) for _ in range(length))  \n\n# Generate a random string of length 20  \n# random_string = generate_random_string()  \n# print(f\"Generated random string: {random_string}\")\n\n\n\nfrom gensim.summarization import summarize  \n\n\n\n\ndef generate_summary(text, word_limit=50):  \n    \"\"\"  \n    使用 gensim 的 TextRank 算法生成文本摘要。  \n    \n    参数:  \n        text (str): 需要进行摘要的文本。  \n        word_limit (int): 概括文本的最大字数。  \n        \n    返回:  \n        summary (str): 返回的文本摘要。  \n    \"\"\"  \n    try:  \n        # 使用 gensim 的 summarize 函数生成摘要  \n        # word_count 指定摘要的最大字数，超过此字数将被切除  \n        summary = summarize(text, word_count=word_limit)  \n        \n        # 如果生成的摘要为空，意味着该段文本可能太短或无法生成有效摘要  \n        if not summary:  \n            return \"无法生成摘要，输入文本可能过短或非标准。\"  \n        \n        return summary  \n    except ValueError as e:  \n        # 在处理异常的地方，确保在文本过短或其他错误时返回信息  \n        return f\"无法生成摘要: {str(e)}\"  \n\n# 示例用法  \ntext = (\"Gensim is an open-source library for unsupervised topic modeling and natural language processing, \"  \n        \"using modern statistical machine learning. It is implemented in Python and Cython for performance.\"  \n        \"Machine learning essentially falls into two categories, supervised and unsupervised learning. \"  \n        \"Gensim specializes in unsupervised learning tasks to extract semantic topics from documents, build document similarity models, etc.\")  \n\nsummary = generate_summary(text)  \nprint(\"Summary:\", summary) \n\n\n# 定义格式化函数  \ndef format_example(example):\n    id = generate_random_string()\n    return {  \n        'id': id,  \n        'article': example['text'],  \n        'highlights': generate_summary(example['text'])  \n    }  \n\n# 使用map函数格式化数据集  \nformatted_train_dataset = train_dataset.map(format_example)  \nformatted_test_dataset = test_dataset.map(format_example)  \n\nformatted_train_dataset = formatted_train_dataset.remove_columns([\"label\", \"label_text\",\"text\"])\nformatted_test_dataset = formatted_test_dataset.remove_columns([\"label\", \"label_text\",\"text\"])\n\n# 保存为JSON Lines格式  \nformatted_train_dataset.to_json(\"/kaggle/working/dataset/bbc_news_formatted_train.json\", orient='records', lines=True)  \nformatted_test_dataset.to_json(\"/kaggle/working/dataset/bbc_news_formatted_test.json\", orient='records', lines=True)  \n\nprint(\"Data saved to bbc_news_formatted_train.json\")\nprint(\"Data saved to bbc_news_formatted_test.json\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:14:39.349603Z","iopub.execute_input":"2024-09-19T08:14:39.350610Z","iopub.status.idle":"2024-09-19T08:14:40.816230Z","shell.execute_reply.started":"2024-09-19T08:14:39.350553Z","shell.execute_reply":"2024-09-19T08:14:40.815297Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Summary: Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning.\nIt is implemented in Python and Cython for performance.Machine learning essentially falls into two categories, supervised and unsupervised learning.\nGensim specializes in unsupervised learning tasks to extract semantic topics from documents, build document similarity models, etc.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e81228a341254dcf92870de1f0a0b86f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5d91cfde9fc42e5bb77bbbb801451d1"}},"metadata":{}},{"name":"stdout","text":"Data saved to bbc_news_formatted_train.json\nData saved to bbc_news_formatted_test.json\n","output_type":"stream"}]},{"cell_type":"code","source":"!head -n 1 /kaggle/working/dataset/bbc_news_formatted_train.jsonl","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:14:53.183222Z","iopub.execute_input":"2024-09-19T08:14:53.184092Z","iopub.status.idle":"2024-09-19T08:14:54.228116Z","shell.execute_reply.started":"2024-09-19T08:14:53.184030Z","shell.execute_reply":"2024-09-19T08:14:54.226862Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"{\"id\":\"LMNgUceV6w8C6AOCV1Bs\",\"article\":\"wales want rugby league training wales could follow england s lead by training with a rugby league club.  england have already had a three-day session with leeds rhinos  and wales are thought to be interested in a similar clinic with rivals st helens. saints coach ian millward has given his approval  but if it does happen it is unlikely to be this season. saints have a week s training in portugal next week  while wales will play england in the opening six nations match on 5 february.  we have had an approach from wales   confirmed a saints spokesman.  it s in the very early stages but it is something we are giving serious consideration to.  st helens  who are proud of their welsh connections  are obvious partners for the welsh rugby union  despite a spat in 2001 over the collapse of kieron cunningham s proposed \\u00a3500 000 move to union side swansea. a similar cross-code deal that took iestyn harris from leeds to cardiff in 2001 did go through  before the talented stand-off returned to the 13-man code with bradford bulls. kel coslett  who famously moved from wales to league in the 1960s  is currently saints  football manager  while clive griffiths - wales  defensive coach - is a former st helens player and is thought to be the man behind the latest initiative. scott gibbs  the former wales and lions centre  played for st helens from 1994-96 and was in the challenge cup-winning team at wembley in 1996.\",\"highlights\":\"england have already had a three-day session with leeds rhinos  and wales are thought to be interested in a similar clinic with rivals st helens.\\nkel coslett  who famously moved from wales to league in the 1960s  is currently saints  football manager  while clive griffiths - wales  defensive coach - is a former st helens player and is thought to be the man behind the latest initiative.\"}\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/transformers","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:14:57.710383Z","iopub.execute_input":"2024-09-19T08:14:57.711373Z","iopub.status.idle":"2024-09-19T08:14:57.718380Z","shell.execute_reply.started":"2024-09-19T08:14:57.711327Z","shell.execute_reply":"2024-09-19T08:14:57.717383Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"/kaggle/working/transformers\n","output_type":"stream"}]},{"cell_type":"code","source":"!python examples/pytorch/summarization/run_summarization.py \\\n    --model_name_or_path google-t5/t5-small \\\n    --do_train \\\n    --do_eval \\\n    --train_file /kaggle/working/dataset/bbc_news_formatted_train.json \\\n    --validation_file /kaggle/working/dataset/bbc_news_formatted_test.json \\\n    --text_column \"article\" \\\n    --summary_column \"highlights\" \\\n    --source_prefix \"summarize: \" \\\n    --output_dir /kaggle/workspace/accelerate1/tst-summarization \\\n    --overwrite_output_dir \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --predict_with_generate","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:15:24.081197Z","iopub.execute_input":"2024-09-19T08:15:24.081961Z","iopub.status.idle":"2024-09-19T08:18:13.125939Z","shell.execute_reply.started":"2024-09-19T08:15:24.081917Z","shell.execute_reply":"2024-09-19T08:18:13.124472Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing custom data configuration default-0d87c217116fcbce\nLoading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\nGenerating dataset json (/root/.cache/huggingface/datasets/json/default-0d87c217116fcbce/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-0d87c217116fcbce/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\nDownloading took 0.0 min\nChecksum Computation took 0.0 min\nGenerating train split\nGenerating train split: 900 examples [00:00, 58773.94 examples/s]\nGenerating validation split\nGenerating validation split: 100 examples [00:00, 29696.29 examples/s]\nUnable to verify splits sizes.\nDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-0d87c217116fcbce/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n[INFO|configuration_utils.py:672] 2024-09-19 08:15:33,561 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n[INFO|configuration_utils.py:739] 2024-09-19 08:15:33,566 >> Model config T5Config {\n  \"_name_or_path\": \"google-t5/t5-small\",\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 512,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 6,\n  \"num_heads\": 8,\n  \"num_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"transformers_version\": \"4.45.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:15:33,654 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:15:33,654 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:15:33,654 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:15:33,654 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:15:33,655 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n[INFO|modeling_utils.py:3702] 2024-09-19 08:15:33,771 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\n[INFO|configuration_utils.py:1097] 2024-09-19 08:15:33,778 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\n[INFO|modeling_utils.py:4544] 2024-09-19 08:15:33,997 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\n[INFO|modeling_utils.py:4552] 2024-09-19 08:15:33,997 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google-t5/t5-small.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n[INFO|configuration_utils.py:1052] 2024-09-19 08:15:34,086 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\n[INFO|configuration_utils.py:1097] 2024-09-19 08:15:34,086 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nRunning tokenizer on train dataset:   0%|        | 0/900 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-0d87c217116fcbce/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-652379d44cdd9dba.arrow\nRunning tokenizer on train dataset: 100%|█| 900/900 [00:02<00:00, 418.56 example\nRunning tokenizer on validation dataset:   0%|   | 0/100 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-0d87c217116fcbce/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-5217167a2dae64f5.arrow\nRunning tokenizer on validation dataset: 100%|█| 100/100 [00:00<00:00, 456.50 ex\n[INFO|trainer.py:2212] 2024-09-19 08:15:38,226 >> ***** Running training *****\n[INFO|trainer.py:2213] 2024-09-19 08:15:38,226 >>   Num examples = 900\n[INFO|trainer.py:2214] 2024-09-19 08:15:38,226 >>   Num Epochs = 3\n[INFO|trainer.py:2215] 2024-09-19 08:15:38,226 >>   Instantaneous batch size per device = 4\n[INFO|trainer.py:2217] 2024-09-19 08:15:38,226 >>   Training with DataParallel so batch size has been adjusted to: 8\n[INFO|trainer.py:2218] 2024-09-19 08:15:38,226 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n[INFO|trainer.py:2219] 2024-09-19 08:15:38,226 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:2220] 2024-09-19 08:15:38,226 >>   Total optimization steps = 339\n[INFO|trainer.py:2221] 2024-09-19 08:15:38,228 >>   Number of trainable parameters = 60,506,624\n  0%|                                                   | 0/339 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|█████████████████████████████████████████| 339/339 [02:13<00:00,  3.00it/s][INFO|trainer.py:3674] 2024-09-19 08:17:52,197 >> Saving model checkpoint to /kaggle/workspace/accelerate1/tst-summarization/checkpoint-339\n[INFO|configuration_utils.py:407] 2024-09-19 08:17:52,200 >> Configuration saved in /kaggle/workspace/accelerate1/tst-summarization/checkpoint-339/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 08:17:52,201 >> Configuration saved in /kaggle/workspace/accelerate1/tst-summarization/checkpoint-339/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 08:17:52,714 >> Model weights saved in /kaggle/workspace/accelerate1/tst-summarization/checkpoint-339/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 08:17:52,717 >> tokenizer config file saved in /kaggle/workspace/accelerate1/tst-summarization/checkpoint-339/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 08:17:52,718 >> Special tokens file saved in /kaggle/workspace/accelerate1/tst-summarization/checkpoint-339/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 08:17:52,719 >> Copy vocab file to /kaggle/workspace/accelerate1/tst-summarization/checkpoint-339/spiece.model\n[INFO|trainer.py:2474] 2024-09-19 08:17:53,442 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 135.214, 'train_samples_per_second': 19.968, 'train_steps_per_second': 2.507, 'train_loss': 0.20907140411106886, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 339/339 [02:15<00:00,  2.51it/s]\n[INFO|trainer.py:3674] 2024-09-19 08:17:53,444 >> Saving model checkpoint to /kaggle/workspace/accelerate1/tst-summarization\n[INFO|configuration_utils.py:407] 2024-09-19 08:17:53,447 >> Configuration saved in /kaggle/workspace/accelerate1/tst-summarization/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 08:17:53,448 >> Configuration saved in /kaggle/workspace/accelerate1/tst-summarization/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 08:17:53,951 >> Model weights saved in /kaggle/workspace/accelerate1/tst-summarization/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 08:17:53,954 >> tokenizer config file saved in /kaggle/workspace/accelerate1/tst-summarization/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 08:17:53,955 >> Special tokens file saved in /kaggle/workspace/accelerate1/tst-summarization/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 08:17:53,956 >> Copy vocab file to /kaggle/workspace/accelerate1/tst-summarization/spiece.model\n***** train metrics *****\n  epoch                    =        3.0\n  total_flos               =   593814GF\n  train_loss               =     0.2091\n  train_runtime            = 0:02:15.21\n  train_samples            =        900\n  train_samples_per_second =     19.968\n  train_steps_per_second   =      2.507\n[INFO|trainer.py:3990] 2024-09-19 08:17:53,973 >> \n***** Running Evaluation *****\n[INFO|trainer.py:3992] 2024-09-19 08:17:53,973 >>   Num examples = 100\n[INFO|trainer.py:3995] 2024-09-19 08:17:53,973 >>   Batch size = 8\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|███████████████████████████████████████████| 13/13 [00:15<00:00,  1.20s/it]\n***** eval metrics *****\n  epoch                   =        3.0\n  eval_gen_len            =       72.7\n  eval_loss               =     0.1186\n  eval_rouge1             =    46.9269\n  eval_rouge2             =    31.5442\n  eval_rougeL             =    38.9266\n  eval_rougeLsum          =    43.0468\n  eval_runtime            = 0:00:17.17\n  eval_samples            =        100\n  eval_samples_per_second =      5.823\n  eval_steps_per_second   =      0.757\n[INFO|modelcard.py:449] 2024-09-19 08:18:11,235 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Summarization', 'type': 'summarization'}, 'metrics': [{'name': 'Rouge1', 'type': 'rouge', 'value': 46.9269}]}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 截断数据集，以进行快速测试","metadata":{}},{"cell_type":"code","source":"!python examples/pytorch/summarization/run_summarization.py \\\n    --model_name_or_path google-t5/t5-small \\\n    --max_train_samples 50 \\\n    --max_eval_samples 50 \\\n    --max_predict_samples 50 \\\n    --do_train \\\n    --do_eval \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --source_prefix \"summarize: \" \\\n    --output_dir /kaggle/workspace/truncation_train/tst-summarization \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --overwrite_output_dir \\\n    --predict_with_generate","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:29:05.195185Z","iopub.execute_input":"2024-09-19T08:29:05.196058Z","iopub.status.idle":"2024-09-19T08:29:29.349818Z","shell.execute_reply.started":"2024-09-19T08:29:05.196015Z","shell.execute_reply":"2024-09-19T08:29:29.348770Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nOverwrite dataset info from restored data version if exists.\nLoading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\nFound cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d)\nLoading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n[INFO|configuration_utils.py:672] 2024-09-19 08:29:17,798 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n[INFO|configuration_utils.py:739] 2024-09-19 08:29:17,802 >> Model config T5Config {\n  \"_name_or_path\": \"google-t5/t5-small\",\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 512,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 6,\n  \"num_heads\": 8,\n  \"num_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"transformers_version\": \"4.45.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:29:17,886 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:29:17,886 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:29:17,886 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:29:17,886 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:29:17,886 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n[INFO|modeling_utils.py:3702] 2024-09-19 08:29:17,998 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\n[INFO|configuration_utils.py:1097] 2024-09-19 08:29:18,005 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\n[INFO|modeling_utils.py:4544] 2024-09-19 08:29:18,224 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\n[INFO|modeling_utils.py:4552] 2024-09-19 08:29:18,224 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google-t5/t5-small.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n[INFO|configuration_utils.py:1052] 2024-09-19 08:29:18,323 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\n[INFO|configuration_utils.py:1097] 2024-09-19 08:29:18,324 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nLoading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-3a4b3d07e7428849.arrow\nLoading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-f06830428f38a2a4.arrow\n[INFO|trainer.py:2212] 2024-09-19 08:29:19,861 >> ***** Running training *****\n[INFO|trainer.py:2213] 2024-09-19 08:29:19,861 >>   Num examples = 50\n[INFO|trainer.py:2214] 2024-09-19 08:29:19,861 >>   Num Epochs = 3\n[INFO|trainer.py:2215] 2024-09-19 08:29:19,861 >>   Instantaneous batch size per device = 4\n[INFO|trainer.py:2217] 2024-09-19 08:29:19,861 >>   Training with DataParallel so batch size has been adjusted to: 8\n[INFO|trainer.py:2218] 2024-09-19 08:29:19,861 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n[INFO|trainer.py:2219] 2024-09-19 08:29:19,861 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:2220] 2024-09-19 08:29:19,861 >>   Total optimization steps = 21\n[INFO|trainer.py:2221] 2024-09-19 08:29:19,862 >>   Number of trainable parameters = 60,506,624\n  0%|                                                    | 0/21 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n 90%|██████████████████████████████████████▉    | 19/21 [00:08<00:00,  2.38it/s]^C\nTraceback (most recent call last):\n  File \"/kaggle/working/transformers/examples/pytorch/summarization/run_summarization.py\", line 773, in <module>\n    main()\n  File \"/kaggle/working/transformers/examples/pytorch/summarization/run_summarization.py\", line 692, in main\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2021, in train\n    return inner_training_loop(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2357, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3487, in training_step\n    self.accelerator.backward(loss, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2237, in backward\n    loss.backward(**kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 521, in backward\n    torch.autograd.backward(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 289, in backward\n    _engine_run_backward(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py\", line 768, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 从检查点恢复训练\n\n- 这将确保您在训练中断时可以从上次停下的地方继续，而无需重新开始。\n  - 第一种方法使用 `output_dir previous_output_dir` 参数从 `output_dir` 中存储的最新检查点恢复训练。在这种情况下，您应该删除 `overwrite_output_dir`\n  - 第二种方法使用 `resume_from_checkpoint path_to_specific_checkpoint` 参数从特定检查点文件夹恢复训练。","metadata":{}},{"cell_type":"markdown","source":"### 方法1\n上面那段代码训练到一半的时候，直接手动停止","metadata":{}},{"cell_type":"code","source":"!python examples/pytorch/summarization/run_summarization.py \\\n    --model_name_or_path google-t5/t5-small \\\n    --max_train_samples 50 \\\n    --max_eval_samples 50 \\\n    --max_predict_samples 50 \\\n    --do_train \\\n    --do_eval \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --source_prefix \"summarize: \" \\\n    --output_dir /kaggle/workspace/truncation_train/tst-summarization \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --output_dir previous_output_dir \\\n    --overwrite_output_dir \\\n    --predict_with_generate","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:30:36.703615Z","iopub.execute_input":"2024-09-19T08:30:36.704410Z","iopub.status.idle":"2024-09-19T08:31:13.587662Z","shell.execute_reply.started":"2024-09-19T08:30:36.704364Z","shell.execute_reply":"2024-09-19T08:31:13.586609Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nOverwrite dataset info from restored data version if exists.\nLoading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\nFound cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d)\nLoading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n[INFO|configuration_utils.py:672] 2024-09-19 08:30:49,522 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n[INFO|configuration_utils.py:739] 2024-09-19 08:30:49,528 >> Model config T5Config {\n  \"_name_or_path\": \"google-t5/t5-small\",\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 512,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 6,\n  \"num_heads\": 8,\n  \"num_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"transformers_version\": \"4.45.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:30:49,606 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:30:49,606 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:30:49,606 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:30:49,606 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:30:49,606 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n[INFO|modeling_utils.py:3702] 2024-09-19 08:30:49,730 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\n[INFO|configuration_utils.py:1097] 2024-09-19 08:30:49,738 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\n[INFO|modeling_utils.py:4544] 2024-09-19 08:30:49,959 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\n[INFO|modeling_utils.py:4552] 2024-09-19 08:30:49,959 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google-t5/t5-small.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n[INFO|configuration_utils.py:1052] 2024-09-19 08:30:50,052 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\n[INFO|configuration_utils.py:1097] 2024-09-19 08:30:50,053 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nLoading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-3a4b3d07e7428849.arrow\nLoading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-f06830428f38a2a4.arrow\n[INFO|trainer.py:2212] 2024-09-19 08:30:51,626 >> ***** Running training *****\n[INFO|trainer.py:2213] 2024-09-19 08:30:51,627 >>   Num examples = 50\n[INFO|trainer.py:2214] 2024-09-19 08:30:51,627 >>   Num Epochs = 3\n[INFO|trainer.py:2215] 2024-09-19 08:30:51,627 >>   Instantaneous batch size per device = 4\n[INFO|trainer.py:2217] 2024-09-19 08:30:51,627 >>   Training with DataParallel so batch size has been adjusted to: 8\n[INFO|trainer.py:2218] 2024-09-19 08:30:51,627 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n[INFO|trainer.py:2219] 2024-09-19 08:30:51,627 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:2220] 2024-09-19 08:30:51,627 >>   Total optimization steps = 21\n[INFO|trainer.py:2221] 2024-09-19 08:30:51,628 >>   Number of trainable parameters = 60,506,624\n  0%|                                                    | 0/21 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|███████████████████████████████████████████| 21/21 [00:09<00:00,  2.85it/s][INFO|trainer.py:3674] 2024-09-19 08:31:00,960 >> Saving model checkpoint to previous_output_dir/checkpoint-21\n[INFO|configuration_utils.py:407] 2024-09-19 08:31:00,962 >> Configuration saved in previous_output_dir/checkpoint-21/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 08:31:00,963 >> Configuration saved in previous_output_dir/checkpoint-21/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 08:31:01,514 >> Model weights saved in previous_output_dir/checkpoint-21/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 08:31:01,517 >> tokenizer config file saved in previous_output_dir/checkpoint-21/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 08:31:01,518 >> Special tokens file saved in previous_output_dir/checkpoint-21/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 08:31:01,519 >> Copy vocab file to previous_output_dir/checkpoint-21/spiece.model\n[INFO|trainer.py:2474] 2024-09-19 08:31:02,451 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 10.8234, 'train_samples_per_second': 13.859, 'train_steps_per_second': 1.94, 'train_loss': 2.3280156453450522, 'epoch': 3.0}\n100%|███████████████████████████████████████████| 21/21 [00:10<00:00,  1.94it/s]\n[INFO|trainer.py:3674] 2024-09-19 08:31:02,454 >> Saving model checkpoint to previous_output_dir\n[INFO|configuration_utils.py:407] 2024-09-19 08:31:02,456 >> Configuration saved in previous_output_dir/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 08:31:02,457 >> Configuration saved in previous_output_dir/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 08:31:02,859 >> Model weights saved in previous_output_dir/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 08:31:02,862 >> tokenizer config file saved in previous_output_dir/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 08:31:02,862 >> Special tokens file saved in previous_output_dir/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 08:31:02,863 >> Copy vocab file to previous_output_dir/spiece.model\n***** train metrics *****\n  epoch                    =        3.0\n  total_flos               =    37796GF\n  train_loss               =      2.328\n  train_runtime            = 0:00:10.82\n  train_samples            =         50\n  train_samples_per_second =     13.859\n  train_steps_per_second   =       1.94\n[INFO|trainer.py:3990] 2024-09-19 08:31:02,876 >> \n***** Running Evaluation *****\n[INFO|trainer.py:3992] 2024-09-19 08:31:02,876 >>   Num examples = 50\n[INFO|trainer.py:3995] 2024-09-19 08:31:02,876 >>   Batch size = 8\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|█████████████████████████████████████████████| 7/7 [00:07<00:00,  1.06s/it]\n***** eval metrics *****\n  epoch                   =        3.0\n  eval_gen_len            =      59.92\n  eval_loss               =     2.1129\n  eval_rouge1             =    34.0571\n  eval_rouge2             =    14.8943\n  eval_rougeL             =    25.6463\n  eval_rougeLsum          =    31.5025\n  eval_runtime            = 0:00:08.87\n  eval_samples            =         50\n  eval_samples_per_second =      5.637\n  eval_steps_per_second   =      0.789\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 方法2","metadata":{}},{"cell_type":"code","source":"!python examples/pytorch/summarization/run_summarization.py \\\n    --model_name_or_path google-t5/t5-small \\\n    --max_train_samples 100 \\\n    --max_eval_samples 100 \\\n    --max_predict_samples 100 \\\n    --do_train \\\n    --do_eval \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --source_prefix \"summarize: \" \\\n    --output_dir /kaggle/workspace/resume_train/tst-summarization \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --overwrite_output_dir \\\n    --resume_from_checkpoint /kaggle/workspace/truncation_train/tst-summarization \\\n    --predict_with_generate","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:32:10.109150Z","iopub.execute_input":"2024-09-19T08:32:10.109551Z","iopub.status.idle":"2024-09-19T08:32:55.910532Z","shell.execute_reply.started":"2024-09-19T08:32:10.109512Z","shell.execute_reply":"2024-09-19T08:32:55.909305Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nOverwrite dataset info from restored data version if exists.\nLoading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\nFound cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d)\nLoading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n[INFO|configuration_utils.py:672] 2024-09-19 08:32:25,055 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n[INFO|configuration_utils.py:739] 2024-09-19 08:32:25,060 >> Model config T5Config {\n  \"_name_or_path\": \"google-t5/t5-small\",\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 512,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 6,\n  \"num_heads\": 8,\n  \"num_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"transformers_version\": \"4.45.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:32:25,171 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:32:25,171 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:32:25,171 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:32:25,171 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:32:25,171 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n[INFO|modeling_utils.py:3702] 2024-09-19 08:32:25,299 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\n[INFO|configuration_utils.py:1097] 2024-09-19 08:32:25,307 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\n[INFO|modeling_utils.py:4544] 2024-09-19 08:32:25,528 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\n[INFO|modeling_utils.py:4552] 2024-09-19 08:32:25,529 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google-t5/t5-small.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n[INFO|configuration_utils.py:1052] 2024-09-19 08:32:25,618 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\n[INFO|configuration_utils.py:1097] 2024-09-19 08:32:25,619 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nRunning tokenizer on train dataset:   0%|        | 0/100 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-9bc3467a177b0625.arrow\nRunning tokenizer on train dataset: 100%|█| 100/100 [00:00<00:00, 322.04 example\nRunning tokenizer on validation dataset:   0%|   | 0/100 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-83c0e15698f0a130.arrow\nRunning tokenizer on validation dataset: 100%|█| 100/100 [00:00<00:00, 361.21 ex\n[INFO|trainer.py:2606] 2024-09-19 08:32:26,942 >> Loading model from /kaggle/workspace/truncation_train/tst-summarization.\n[WARNING|trainer.py:2834] 2024-09-19 08:32:27,037 >> There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n[INFO|trainer.py:2212] 2024-09-19 08:32:27,858 >> ***** Running training *****\n[INFO|trainer.py:2213] 2024-09-19 08:32:27,858 >>   Num examples = 100\n[INFO|trainer.py:2214] 2024-09-19 08:32:27,858 >>   Num Epochs = 3\n[INFO|trainer.py:2215] 2024-09-19 08:32:27,858 >>   Instantaneous batch size per device = 4\n[INFO|trainer.py:2217] 2024-09-19 08:32:27,858 >>   Training with DataParallel so batch size has been adjusted to: 8\n[INFO|trainer.py:2218] 2024-09-19 08:32:27,858 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n[INFO|trainer.py:2219] 2024-09-19 08:32:27,859 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:2220] 2024-09-19 08:32:27,859 >>   Total optimization steps = 39\n[INFO|trainer.py:2221] 2024-09-19 08:32:27,860 >>   Number of trainable parameters = 60,506,624\n[INFO|trainer.py:2243] 2024-09-19 08:32:27,860 >>   Continuing training from checkpoint, will skip to saved global_step\n[INFO|trainer.py:2244] 2024-09-19 08:32:27,860 >>   Continuing training from epoch 1\n[INFO|trainer.py:2245] 2024-09-19 08:32:27,860 >>   Continuing training from global step 21\n[INFO|trainer.py:2247] 2024-09-19 08:32:27,860 >>   Will skip the first 1 epochs then the first 8 batches in the first epoch.\n  0%|                                                    | 0/39 [00:00<?, ?it/s][INFO|trainer.py:2907] 2024-09-19 08:32:27,901 >> Didn't find an RNG file, if you are resuming a training that was launched in a distributed fashion, reproducibility is not guaranteed.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|███████████████████████████████████████████| 39/39 [00:08<00:00,  2.68it/s][INFO|trainer.py:3674] 2024-09-19 08:32:36,286 >> Saving model checkpoint to /kaggle/workspace/resume_train/tst-summarization/checkpoint-39\n[INFO|configuration_utils.py:407] 2024-09-19 08:32:36,288 >> Configuration saved in /kaggle/workspace/resume_train/tst-summarization/checkpoint-39/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 08:32:36,289 >> Configuration saved in /kaggle/workspace/resume_train/tst-summarization/checkpoint-39/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 08:32:36,826 >> Model weights saved in /kaggle/workspace/resume_train/tst-summarization/checkpoint-39/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 08:32:36,829 >> tokenizer config file saved in /kaggle/workspace/resume_train/tst-summarization/checkpoint-39/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 08:32:36,830 >> Special tokens file saved in /kaggle/workspace/resume_train/tst-summarization/checkpoint-39/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 08:32:36,831 >> Copy vocab file to /kaggle/workspace/resume_train/tst-summarization/checkpoint-39/spiece.model\n[INFO|trainer.py:2474] 2024-09-19 08:32:37,575 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 9.7156, 'train_samples_per_second': 30.878, 'train_steps_per_second': 4.014, 'train_loss': 1.0188585917154949, 'epoch': 3.0}\n100%|███████████████████████████████████████████| 39/39 [00:09<00:00,  4.02it/s]\n[INFO|trainer.py:3674] 2024-09-19 08:32:37,578 >> Saving model checkpoint to /kaggle/workspace/resume_train/tst-summarization\n[INFO|configuration_utils.py:407] 2024-09-19 08:32:37,580 >> Configuration saved in /kaggle/workspace/resume_train/tst-summarization/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 08:32:37,581 >> Configuration saved in /kaggle/workspace/resume_train/tst-summarization/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 08:32:38,113 >> Model weights saved in /kaggle/workspace/resume_train/tst-summarization/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 08:32:38,116 >> tokenizer config file saved in /kaggle/workspace/resume_train/tst-summarization/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 08:32:38,117 >> Special tokens file saved in /kaggle/workspace/resume_train/tst-summarization/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 08:32:38,118 >> Copy vocab file to /kaggle/workspace/resume_train/tst-summarization/spiece.model\n***** train metrics *****\n  epoch                    =        3.0\n  total_flos               =    71746GF\n  train_loss               =     1.0189\n  train_runtime            = 0:00:09.71\n  train_samples            =        100\n  train_samples_per_second =     30.878\n  train_steps_per_second   =      4.014\n[INFO|trainer.py:3990] 2024-09-19 08:32:38,132 >> \n***** Running Evaluation *****\n[INFO|trainer.py:3992] 2024-09-19 08:32:38,132 >>   Num examples = 100\n[INFO|trainer.py:3995] 2024-09-19 08:32:38,132 >>   Batch size = 8\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|███████████████████████████████████████████| 13/13 [00:14<00:00,  1.13s/it]\n***** eval metrics *****\n  epoch                   =        3.0\n  eval_gen_len            =      59.46\n  eval_loss               =     2.0849\n  eval_rouge1             =    31.7071\n  eval_rouge2             =     12.343\n  eval_rougeL             =    23.4189\n  eval_rougeLsum          =    28.6133\n  eval_runtime            = 0:00:15.90\n  eval_samples            =        100\n  eval_samples_per_second =      6.286\n  eval_steps_per_second   =      0.817\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 分享微调好的模型到HuggingFace","metadata":{}},{"cell_type":"code","source":"# kaggle上无法使用这种方案\n\n# !huggingface-cli login","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:37:29.549558Z","iopub.execute_input":"2024-09-19T08:37:29.550551Z","iopub.status.idle":"2024-09-19T08:37:31.036331Z","shell.execute_reply.started":"2024-09-19T08:37:29.550508Z","shell.execute_reply":"2024-09-19T08:37:31.035151Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"usage: huggingface-cli <command> [<args>]\nhuggingface-cli: error: unrecognized arguments: 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\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub.hf_api import HfFolder\n\nHfFolder.save_token('xxxxxxxxx')\n\nprint(\"successfully login into huggingface ~\")\n\n# 请确保你的huggingface role具有 写权限","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:52:07.731644Z","iopub.execute_input":"2024-09-19T08:52:07.732687Z","iopub.status.idle":"2024-09-19T08:52:07.738931Z","shell.execute_reply.started":"2024-09-19T08:52:07.732641Z","shell.execute_reply":"2024-09-19T08:52:07.737997Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"successfully login into huggingface ~\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 然后将 push_to_hub 参数添加到脚本中。此参数将使用您的 Hugging Face 用户名和 output_dir 中指定的文件夹名称创建一个存储库。\n### 要为您的存储库指定特定名称，请使用 push_to_hub_model_id 参数添加它。该存储库将自动列在您的命名空间下。","metadata":{}},{"cell_type":"code","source":"!python examples/pytorch/summarization/run_summarization.py \\\n    --model_name_or_path google-t5/t5-small \\\n    --max_train_samples 2000 \\\n    --max_eval_samples 2000 \\\n    --max_predict_samples 2000 \\\n    --do_train \\\n    --do_eval \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --source_prefix \"summarize: \" \\\n    --push_to_hub \\\n    --push_to_hub_model_id finetuned-t5-cnn_dailymail \\\n    --output_dir /kaggle/workspace/train2000/tst-summarization \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --overwrite_output_dir \\\n    --predict_with_generate","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:53:42.861468Z","iopub.execute_input":"2024-09-19T08:53:42.862436Z","iopub.status.idle":"2024-09-19T09:04:58.681241Z","shell.execute_reply.started":"2024-09-19T08:53:42.862389Z","shell.execute_reply":"2024-09-19T09:04:58.680015Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:2042: FutureWarning: `--push_to_hub_model_id` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case BruceNju/finetuned-t5-cnn_dailymail).\n  warnings.warn(\nOverwrite dataset info from restored data version if exists.\nLoading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\nFound cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d)\nLoading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n[INFO|configuration_utils.py:672] 2024-09-19 08:53:55,986 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n[INFO|configuration_utils.py:739] 2024-09-19 08:53:55,990 >> Model config T5Config {\n  \"_name_or_path\": \"google-t5/t5-small\",\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 512,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 6,\n  \"num_heads\": 8,\n  \"num_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"transformers_version\": \"4.45.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:53:56,074 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:53:56,074 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:53:56,074 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:53:56,074 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2215] 2024-09-19 08:53:56,074 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n[INFO|modeling_utils.py:3702] 2024-09-19 08:53:56,189 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\n[INFO|configuration_utils.py:1097] 2024-09-19 08:53:56,196 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\n[INFO|modeling_utils.py:4544] 2024-09-19 08:53:56,410 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\n[INFO|modeling_utils.py:4552] 2024-09-19 08:53:56,410 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google-t5/t5-small.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n[INFO|configuration_utils.py:1052] 2024-09-19 08:53:56,519 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\n[INFO|configuration_utils.py:1097] 2024-09-19 08:53:56,520 >> Generate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nRunning tokenizer on train dataset:   0%|       | 0/2000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-65acac57b5d70083.arrow\nRunning tokenizer on train dataset: 100%|█| 2000/2000 [00:06<00:00, 292.35 examp\nRunning tokenizer on validation dataset:   0%|  | 0/2000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-f3c3226bc5243e66.arrow\nRunning tokenizer on validation dataset: 100%|█| 2000/2000 [00:06<00:00, 293.67 \n[INFO|trainer.py:2212] 2024-09-19 08:54:11,985 >> ***** Running training *****\n[INFO|trainer.py:2213] 2024-09-19 08:54:11,986 >>   Num examples = 2,000\n[INFO|trainer.py:2214] 2024-09-19 08:54:11,986 >>   Num Epochs = 3\n[INFO|trainer.py:2215] 2024-09-19 08:54:11,986 >>   Instantaneous batch size per device = 4\n[INFO|trainer.py:2217] 2024-09-19 08:54:11,986 >>   Training with DataParallel so batch size has been adjusted to: 8\n[INFO|trainer.py:2218] 2024-09-19 08:54:11,986 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n[INFO|trainer.py:2219] 2024-09-19 08:54:11,986 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:2220] 2024-09-19 08:54:11,986 >>   Total optimization steps = 750\n[INFO|trainer.py:2221] 2024-09-19 08:54:11,987 >>   Number of trainable parameters = 60,506,624\n  0%|                                                   | 0/750 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n{'loss': 2.0135, 'grad_norm': 2.001534938812256, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n 67%|███████████████████████████▎             | 500/750 [03:37<01:46,  2.34it/s][INFO|trainer.py:3674] 2024-09-19 08:57:49,356 >> Saving model checkpoint to /kaggle/workspace/train2000/tst-summarization/checkpoint-500\n[INFO|configuration_utils.py:407] 2024-09-19 08:57:49,358 >> Configuration saved in /kaggle/workspace/train2000/tst-summarization/checkpoint-500/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 08:57:49,359 >> Configuration saved in /kaggle/workspace/train2000/tst-summarization/checkpoint-500/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 08:57:49,883 >> Model weights saved in /kaggle/workspace/train2000/tst-summarization/checkpoint-500/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 08:57:49,886 >> tokenizer config file saved in /kaggle/workspace/train2000/tst-summarization/checkpoint-500/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 08:57:49,886 >> Special tokens file saved in /kaggle/workspace/train2000/tst-summarization/checkpoint-500/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 08:57:49,887 >> Copy vocab file to /kaggle/workspace/train2000/tst-summarization/checkpoint-500/spiece.model\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 08:57:51,000 >> tokenizer config file saved in /kaggle/workspace/train2000/tst-summarization/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 08:57:51,001 >> Special tokens file saved in /kaggle/workspace/train2000/tst-summarization/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 08:57:51,002 >> Copy vocab file to /kaggle/workspace/train2000/tst-summarization/spiece.model\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|█████████████████████████████████████████| 750/750 [05:26<00:00,  2.35it/s][INFO|trainer.py:3674] 2024-09-19 08:59:38,598 >> Saving model checkpoint to /kaggle/workspace/train2000/tst-summarization/checkpoint-750\n[INFO|configuration_utils.py:407] 2024-09-19 08:59:38,601 >> Configuration saved in /kaggle/workspace/train2000/tst-summarization/checkpoint-750/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 08:59:38,602 >> Configuration saved in /kaggle/workspace/train2000/tst-summarization/checkpoint-750/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 08:59:39,131 >> Model weights saved in /kaggle/workspace/train2000/tst-summarization/checkpoint-750/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 08:59:39,134 >> tokenizer config file saved in /kaggle/workspace/train2000/tst-summarization/checkpoint-750/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 08:59:39,134 >> Special tokens file saved in /kaggle/workspace/train2000/tst-summarization/checkpoint-750/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 08:59:39,135 >> Copy vocab file to /kaggle/workspace/train2000/tst-summarization/checkpoint-750/spiece.model\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 08:59:40,665 >> tokenizer config file saved in /kaggle/workspace/train2000/tst-summarization/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 08:59:40,665 >> Special tokens file saved in /kaggle/workspace/train2000/tst-summarization/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 08:59:40,668 >> Copy vocab file to /kaggle/workspace/train2000/tst-summarization/spiece.model\n[INFO|trainer.py:2474] 2024-09-19 08:59:40,698 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 328.7104, 'train_samples_per_second': 18.253, 'train_steps_per_second': 2.282, 'train_loss': 1.9870933024088542, 'epoch': 3.0}\n100%|█████████████████████████████████████████| 750/750 [05:28<00:00,  2.28it/s]\n[INFO|trainer.py:4456] 2024-09-19 08:59:40,701 >> Waiting for the current checkpoint push to be finished, this might take a couple of minutes.\n[INFO|trainer.py:3674] 2024-09-19 08:59:51,168 >> Saving model checkpoint to /kaggle/workspace/train2000/tst-summarization\n[INFO|configuration_utils.py:407] 2024-09-19 08:59:51,171 >> Configuration saved in /kaggle/workspace/train2000/tst-summarization/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 08:59:51,172 >> Configuration saved in /kaggle/workspace/train2000/tst-summarization/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 08:59:51,958 >> Model weights saved in /kaggle/workspace/train2000/tst-summarization/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 08:59:51,961 >> tokenizer config file saved in /kaggle/workspace/train2000/tst-summarization/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 08:59:51,962 >> Special tokens file saved in /kaggle/workspace/train2000/tst-summarization/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 08:59:51,964 >> Copy vocab file to /kaggle/workspace/train2000/tst-summarization/spiece.model\n[INFO|trainer.py:3674] 2024-09-19 08:59:51,979 >> Saving model checkpoint to /kaggle/workspace/train2000/tst-summarization\n[INFO|configuration_utils.py:407] 2024-09-19 08:59:51,982 >> Configuration saved in /kaggle/workspace/train2000/tst-summarization/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 08:59:51,982 >> Configuration saved in /kaggle/workspace/train2000/tst-summarization/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 08:59:52,875 >> Model weights saved in /kaggle/workspace/train2000/tst-summarization/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 08:59:52,878 >> tokenizer config file saved in /kaggle/workspace/train2000/tst-summarization/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 08:59:52,878 >> Special tokens file saved in /kaggle/workspace/train2000/tst-summarization/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 08:59:52,881 >> Copy vocab file to /kaggle/workspace/train2000/tst-summarization/spiece.model\n[INFO|modelcard.py:449] 2024-09-19 08:59:53,025 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}}\n***** train metrics *****\n  epoch                    =        3.0\n  total_flos               =  1508570GF\n  train_loss               =     1.9871\n  train_runtime            = 0:05:28.71\n  train_samples            =       2000\n  train_samples_per_second =     18.253\n  train_steps_per_second   =      2.282\n[INFO|trainer.py:3990] 2024-09-19 08:59:54,974 >> \n***** Running Evaluation *****\n[INFO|trainer.py:3992] 2024-09-19 08:59:54,974 >>   Num examples = 2000\n[INFO|trainer.py:3995] 2024-09-19 08:59:54,974 >>   Batch size = 8\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|█████████████████████████████████████████| 250/250 [04:56<00:00,  1.19s/it]\n***** eval metrics *****\n  epoch                   =        3.0\n  eval_gen_len            =    57.9425\n  eval_loss               =     1.9668\n  eval_rouge1             =    34.6768\n  eval_rouge2             =    13.9858\n  eval_rougeL             =    24.7762\n  eval_rougeLsum          =    32.0487\n  eval_runtime            = 0:04:58.13\n  eval_samples            =       2000\n  eval_samples_per_second =      6.708\n  eval_steps_per_second   =      0.839\n[INFO|trainer.py:3674] 2024-09-19 09:04:53,115 >> Saving model checkpoint to /kaggle/workspace/train2000/tst-summarization\n[INFO|configuration_utils.py:407] 2024-09-19 09:04:53,118 >> Configuration saved in /kaggle/workspace/train2000/tst-summarization/config.json\n[INFO|configuration_utils.py:866] 2024-09-19 09:04:53,118 >> Configuration saved in /kaggle/workspace/train2000/tst-summarization/generation_config.json\n[INFO|modeling_utils.py:2806] 2024-09-19 09:04:54,077 >> Model weights saved in /kaggle/workspace/train2000/tst-summarization/model.safetensors\n[INFO|tokenization_utils_base.py:2650] 2024-09-19 09:04:54,080 >> tokenizer config file saved in /kaggle/workspace/train2000/tst-summarization/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2659] 2024-09-19 09:04:54,080 >> Special tokens file saved in /kaggle/workspace/train2000/tst-summarization/special_tokens_map.json\n[INFO|tokenization_t5_fast.py:175] 2024-09-19 09:04:54,082 >> Copy vocab file to /kaggle/workspace/train2000/tst-summarization/spiece.model\nevents.out.tfevents.1726736693.4fa5516cabac.40491.1: 100%|█| 613/613 [00:00<00:0\n","output_type":"stream"}]}]}